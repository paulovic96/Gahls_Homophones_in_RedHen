{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from english_contractions import ENGLISH_CONTRACTIONS\n",
    "import merging_dataframes\n",
    "import word_pronunciation_predictibility\n",
    "import celex_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "celex_dict_file = \"Data/english/epw/epw.cd\" #\"/mnt/shared/corpora/Celex/english/epw/epw.cd\"\n",
    "filename = \"Data/2016_all_words_no_audio.pickle\" #\"/mnt/Restricted/Corpora/RedHen/2016_all_words_no_audio.pickle\"\n",
    "hom_filename = \"Data/hom.csv\" # \"/mnt/Restricted/Corpora/RedHen/hom.csv\"\n",
    "berndt_character_coding_file = \"Data/phonetic_character_code_berndt1987.csv\" # \"/mnt/Restricted/Corpora/RedHen/phonetic_character_code_berndt1987.csv\"\n",
    "berndt_conditional_probs_file = \"Data/Conditional_Probabilities_for_Grapheme-to-Phoneme_Correspondences_Berndt1987.csv\" # \"/mnt/Restricted/Corpora/RedHen/Conditional_Probabilities_for_Grapheme-to-Phoneme_Correspondences_Berndt1987.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled RedHen Dataframe \n",
    "## Preprocessing:\n",
    "- include pause information\n",
    "- word duration\n",
    "- word frequency\n",
    "- length in letter\n",
    "- contextual predictiaility given prev and next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read dataframe from Data/2016_all_words_no_audio.pickle\n",
      "Preprocessing: extract pause information...\n",
      "Remove pauses from data!\n",
      "Preprocessing: apply word preprocessing...\n",
      "Preprocessing: calculate word duration...\n",
      "Preprocessing: calculate word frequency...\n",
      "Preprocessing: extract context information...\n",
      "Preprocessing: calculate length in letter...\n",
      "Preprocessing: calculate contextual predictability...\n",
      "(18864660, 25) RangeIndex(start=0, stop=18864660, step=1)\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing.read_dataframe(filename, remove_pauses=True, remove_errors=True, preprocessing=True, drop_error_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = [\"2016-12-17_1330_US_KCET_Asia_Insight\", \"2016-10-25_2300_US_KABC_Eyewitness_News_4PM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[df[\"source_file\"].isin(source_files)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_file', 'word', 'start', 'end', 'duration', 'label_type',\n",
       "       'mp4_error', 'aac_error', 'aac2wav_error', 'eafgz_error', 'seg_error',\n",
       "       'preceding_pause', 'subsequent_pause', 'word_frequency', 'prev_word',\n",
       "       'prev_word_frequency', 'next_word', 'next_word_frequency',\n",
       "       'length_in_letter', 'prev_word_string', 'next_word_string',\n",
       "       'prev_word_string_frequency', 'next_word_string_frequency',\n",
       "       'cond_pred_prev', 'cond_pred_next'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gahls Homophones extracted from RedHen Dataframe\n",
    "## Preprocessing:\n",
    "- is_pair for indicating whether homophones found in data have a matching pair \n",
    "- is_max factor for indicating most frequent homophone of pair (if not a pair always 1)\n",
    "- pronunciation given by celex encoding and unbounded disc encoding (celexPhon)\n",
    "- add further celex information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read Gahls Homophone data from Data/hom.csv\n",
      "406 out of 412 homophones found in Data:\n",
      "Homophone Pairs found in Data: 200\n",
      "Homophones without Pair:  ['flowers', 'holes', 'moose', 'naval', 'pairs', 'taught']\n",
      "Missing homophones: ['flours' 'mousse' 'navel' 'pears' 'taut' 'wholes']\n"
     ]
    }
   ],
   "source": [
    "homophones_in_data, gahls_homophones, gahls_homophones_missing_in_data = preprocessing.read_and_extract_homophones(hom_filename, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "celex_dict = word_pronunciation_predictibility.get_english_phonology_from_celex(celex_dict_file)\n",
    "homophones_in_data_celex_merged = merging_dataframes.get_celex_transcription(homophones_in_data, celex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_file', 'word', 'start', 'end', 'duration', 'label_type',\n",
       "       'mp4_error', 'aac_error', 'aac2wav_error', 'eafgz_error', 'seg_error',\n",
       "       'preceding_pause', 'subsequent_pause', 'word_frequency', 'prev_word',\n",
       "       'prev_word_frequency', 'next_word', 'next_word_frequency',\n",
       "       'length_in_letter', 'prev_word_string', 'next_word_string',\n",
       "       'prev_word_string_frequency', 'next_word_string_frequency',\n",
       "       'cond_pred_prev', 'cond_pred_next', 'has_pair', 'pron', 'celexPhon',\n",
       "       'pron_frequency', 'is_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones_in_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_file', 'word', 'start', 'end', 'duration', 'label_type',\n",
       "       'mp4_error', 'aac_error', 'aac2wav_error', 'eafgz_error', 'seg_error',\n",
       "       'preceding_pause', 'subsequent_pause', 'word_frequency', 'prev_word',\n",
       "       'prev_word_frequency', 'next_word', 'next_word_frequency',\n",
       "       'length_in_letter', 'prev_word_string', 'next_word_string',\n",
       "       'prev_word_string_frequency', 'next_word_string_frequency',\n",
       "       'cond_pred_prev', 'cond_pred_next', 'has_pair', 'pron', 'celexPhon',\n",
       "       'pron_frequency', 'is_max', 'disc', 'clx', 'disc_no_bound',\n",
       "       'clx_no_bound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones_in_data_celex_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['spell', 'pron', 'lgPronCelFq', 'logCelFq', 'logAvgDur', 'stem',\n",
       "       'is_complex', 'celexPhon', 'phonNeighCount', 'NearestSemNeighCor',\n",
       "       'MeanCorTop20', 'AvCor', 'MedianCor', 'MeanCorTop20Unrel',\n",
       "       'CossinTwinsStem', 'CossinTwinsFull', 'L2Ldiag', 'EuclidDistTwins',\n",
       "       'SL1norm', 'CorrectLDLpred', 'SumChatWord', 'MinChatWord', 'L1ChatWord',\n",
       "       'CorPredWord', 'LWLinkRatioWord', 'RankProd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gahls_homophones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spell</th>\n",
       "      <th>pron</th>\n",
       "      <th>lgPronCelFq</th>\n",
       "      <th>logCelFq</th>\n",
       "      <th>logAvgDur</th>\n",
       "      <th>stem</th>\n",
       "      <th>is_complex</th>\n",
       "      <th>celexPhon</th>\n",
       "      <th>phonNeighCount</th>\n",
       "      <th>NearestSemNeighCor</th>\n",
       "      <th>...</th>\n",
       "      <th>L2Ldiag</th>\n",
       "      <th>EuclidDistTwins</th>\n",
       "      <th>SL1norm</th>\n",
       "      <th>CorrectLDLpred</th>\n",
       "      <th>SumChatWord</th>\n",
       "      <th>MinChatWord</th>\n",
       "      <th>L1ChatWord</th>\n",
       "      <th>CorPredWord</th>\n",
       "      <th>LWLinkRatioWord</th>\n",
       "      <th>RankProd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10963</th>\n",
       "      <td>gym</td>\n",
       "      <td>_Im</td>\n",
       "      <td>6.520621</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>-1.007354</td>\n",
       "      <td>gym</td>\n",
       "      <td>False</td>\n",
       "      <td>_Im</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19868</th>\n",
       "      <td>jim</td>\n",
       "      <td>_Im</td>\n",
       "      <td>6.520621</td>\n",
       "      <td>6.406880</td>\n",
       "      <td>-1.302760</td>\n",
       "      <td>Jim</td>\n",
       "      <td>True</td>\n",
       "      <td>_Im</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>aide</td>\n",
       "      <td>1d</td>\n",
       "      <td>7.020191</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>-0.685675</td>\n",
       "      <td>aide</td>\n",
       "      <td>False</td>\n",
       "      <td>1d</td>\n",
       "      <td>37</td>\n",
       "      <td>0.582451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098318</td>\n",
       "      <td>0.050616</td>\n",
       "      <td>0.703443</td>\n",
       "      <td>True</td>\n",
       "      <td>1.479237</td>\n",
       "      <td>0.495043</td>\n",
       "      <td>1.479237</td>\n",
       "      <td>0.282871</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>aid</td>\n",
       "      <td>1d</td>\n",
       "      <td>7.020191</td>\n",
       "      <td>6.926577</td>\n",
       "      <td>-1.168132</td>\n",
       "      <td>aid</td>\n",
       "      <td>False</td>\n",
       "      <td>1d</td>\n",
       "      <td>37</td>\n",
       "      <td>0.879444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601393</td>\n",
       "      <td>0.050616</td>\n",
       "      <td>2.039614</td>\n",
       "      <td>True</td>\n",
       "      <td>1.250363</td>\n",
       "      <td>0.250344</td>\n",
       "      <td>1.250363</td>\n",
       "      <td>0.913718</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>aides</td>\n",
       "      <td>1dz</td>\n",
       "      <td>5.262690</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>-0.770658</td>\n",
       "      <td>aide</td>\n",
       "      <td>True</td>\n",
       "      <td>1dz</td>\n",
       "      <td>17</td>\n",
       "      <td>0.582451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098318</td>\n",
       "      <td>0.050616</td>\n",
       "      <td>1.584693</td>\n",
       "      <td>True</td>\n",
       "      <td>1.988257</td>\n",
       "      <td>0.494036</td>\n",
       "      <td>1.988257</td>\n",
       "      <td>0.625906</td>\n",
       "      <td>6.122449</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       spell pron  lgPronCelFq  logCelFq  logAvgDur  stem  is_complex  \\\n",
       "10963    gym  _Im     6.520621  4.290459  -1.007354   gym       False   \n",
       "19868    jim  _Im     6.520621  6.406880  -1.302760   Jim        True   \n",
       "331     aide   1d     7.020191  4.605170  -0.685675  aide       False   \n",
       "255      aid   1d     7.020191  6.926577  -1.168132   aid       False   \n",
       "336    aides  1dz     5.262690  4.330733  -0.770658  aide        True   \n",
       "\n",
       "      celexPhon  phonNeighCount  NearestSemNeighCor  ...   L2Ldiag  \\\n",
       "10963       _Im              17                 NaN  ...  0.096807   \n",
       "19868       _Im              17                 NaN  ...       NaN   \n",
       "331          1d              37            0.582451  ...  0.098318   \n",
       "255          1d              37            0.879444  ...  0.601393   \n",
       "336         1dz              17            0.582451  ...  0.098318   \n",
       "\n",
       "       EuclidDistTwins   SL1norm  CorrectLDLpred  SumChatWord  MinChatWord  \\\n",
       "10963              NaN       NaN             NaN          NaN          NaN   \n",
       "19868              NaN       NaN             NaN          NaN          NaN   \n",
       "331           0.050616  0.703443            True     1.479237     0.495043   \n",
       "255           0.050616  2.039614            True     1.250363     0.250344   \n",
       "336           0.050616  1.584693            True     1.988257     0.494036   \n",
       "\n",
       "       L1ChatWord  CorPredWord  LWLinkRatioWord RankProd  \n",
       "10963         NaN          NaN              NaN      NaN  \n",
       "19868         NaN          NaN              NaN      NaN  \n",
       "331      1.479237     0.282871         4.000000      1.0  \n",
       "255      1.250363     0.913718         8.000000      1.0  \n",
       "336      1.988257     0.625906         6.122449      1.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gahls_homophones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load additional information\n",
    "- eaf files\n",
    "- seg files\n",
    "- gentle files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EAF files\n",
    "- information about present gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and extract information from eaf files...\n"
     ]
    }
   ],
   "source": [
    "eaf_data = preprocessing.get_additional_data_from_files(sub_df, \"eaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEG files\n",
    "- information about Part Of Speech\n",
    "- information about Phrase final marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and extract information from seg files...\n"
     ]
    }
   ],
   "source": [
    "seg_data = preprocessing.get_additional_data_from_files(sub_df, \"seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENTLE files\n",
    "- information about Phrase final marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and extract information from gentle files...\n"
     ]
    }
   ],
   "source": [
    "gentle_data = preprocessing.get_additional_data_from_files(sub_df, \"gentle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video files\n",
    "- information about entropy of situation in which the homophones was articulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and extract information from video files...\n"
     ]
    }
   ],
   "source": [
    "video_data = preprocessing.get_additional_data_from_files(homophones_in_data_celex_merged, \"video\") # only for homophones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'celex_files' from '/Users/paule/Desktop/Gahls_Homophones_in_RedHen/celex_files.py'>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(celex_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "celex_data = celex_files.get_syl_counts(celex_files.read_celex_file())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Pronunciation Predictability (Berndt et al. 1987)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Berndt's tables for Phoneme Equivalents and Conditional Probabilities for Grapheme-to-Phoneme Correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "berndt_character_code_df = pd.read_csv(berndt_character_coding_file, delimiter=\";\")\n",
    "berndt_conditional_probs = pd.read_csv(berndt_conditional_probs_file,delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPABET to corresponding Keyboard Compatible Phonemic (KCP) symbol dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "berndt_arpabet_phon_dict = word_pronunciation_predictibility.get_ARPABET_to_keyboard_phonetic_symbols_dict(berndt_character_code_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KCP to Grapheme Symbols and Probabilities dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonem_graphem_prob_dict = word_pronunciation_predictibility.get_keyboard_phonetic_symbols_to_grapheme_cond_prob_dict(berndt_conditional_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homophones with corresponding ARPABET transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_homophones = np.unique(homophones_in_data.word)\n",
    "hom_arpabet_words = word_pronunciation_predictibility.get_ARPABET_phonetic_transcription(unique_homophones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homophones with corresponding KCP transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_kcp_word_tuples = []\n",
    "for i,arpabet_word in enumerate(hom_arpabet_words):\n",
    "    kcp_word = word_pronunciation_predictibility.get_keyboard_phonetic_symbols_for_ARPABET(arpabet_word, berndt_arpabet_phon_dict)\n",
    "    #print(unique_homophones[i],arpabet_word,kcp_word)\n",
    "    hom_kcp_word_tuples.append((unique_homophones[i],kcp_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ad', ['ae', 'd']),\n",
       " ('add', ['ae', 'd']),\n",
       " ('adds', ['ae', 'd', 'z']),\n",
       " ('ads', ['ae', 'd', 'z']),\n",
       " ('aid', ['ay', 'd']),\n",
       " ('aide', ['ay', 'd']),\n",
       " ('aides', ['ay', 'd', 'z']),\n",
       " ('aids', ['ay', 'd', 'z']),\n",
       " ('airs', ['eh', 'r', 'z']),\n",
       " ('allowed', [['ul', ['uh-', 'l']], 'au', 'd']),\n",
       " ('aloud', [['ul', ['uh-', 'l']], 'au', 'd']),\n",
       " ('bail', ['b', 'ay', 'l']),\n",
       " ('baits', ['b', 'ay', 't', 's']),\n",
       " ('bald', ['b', 'aw', 'l', 'd']),\n",
       " ('bale', ['b', 'ay', 'l']),\n",
       " ('band', ['b', 'ae', 'n', 'd']),\n",
       " ('banned', ['b', 'ae', 'n', 'd']),\n",
       " ('bare', ['b', 'eh', 'r']),\n",
       " ('bates', ['b', 'ay', 't', 's']),\n",
       " ('bawled', ['b', 'aw', 'l', 'd']),\n",
       " ('bear', ['b', 'eh', 'r']),\n",
       " ('beats', ['b', 'ee', 't', 's']),\n",
       " ('beets', ['b', 'ee', 't', 's']),\n",
       " ('bell', ['b', 'eh', 'l']),\n",
       " ('belle', ['b', 'eh', 'l']),\n",
       " ('berry', ['b', 'eh', 'r', 'ee']),\n",
       " ('billed', ['b', ['ih', ['ee']], 'l', 'd']),\n",
       " ('blew', ['b', 'l', 'oo']),\n",
       " ('blue', ['b', 'l', 'oo']),\n",
       " ('boar', ['b', ['aw', ['o']], 'r']),\n",
       " ('board', ['b', ['aw', ['o']], 'r', 'd']),\n",
       " ('bold', ['b', 'o', 'l', 'd']),\n",
       " ('bore', ['b', ['aw', ['o']], 'r']),\n",
       " ('bored', ['b', ['aw', ['o']], 'r', 'd']),\n",
       " ('bowled', ['b', 'o', 'l', 'd']),\n",
       " ('brakes', ['b', 'r', 'ay', ['ks', ['k', 's']]]),\n",
       " ('bread', ['b', 'r', 'eh', 'd']),\n",
       " ('breaks', ['b', 'r', 'ay', ['ks', ['k', 's']]]),\n",
       " ('bred', ['b', 'r', 'eh', 'd']),\n",
       " ('build', ['b', ['ih', ['ee']], 'l', 'd']),\n",
       " ('bury', ['b', 'eh', 'r', 'ee']),\n",
       " ('callous', ['k', 'ae', 'l', 'uh-', 's']),\n",
       " ('callus', ['k', 'ae', 'l', 'uh-', 's']),\n",
       " ('capital', ['k', 'ae', 'p', 'uh-', 't', ['ul', ['uh-', 'l']]]),\n",
       " ('capitol', ['k', 'ae', 'p', 'ih', 't', ['ul', ['uh-', 'l']]]),\n",
       " ('ceiling', ['s', 'ee', 'l', 'ih', 'ng']),\n",
       " ('cell', ['s', 'eh', 'l']),\n",
       " ('cellar', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']]]),\n",
       " ('cellars', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']], 'z']),\n",
       " ('cells', ['s', 'eh', 'l', 'z']),\n",
       " ('cent', ['s', 'eh', 'n', 't']),\n",
       " ('cents', ['s', 'eh', 'n', 't', 's']),\n",
       " ('cereal', ['s', ['ih', ['ee']], 'r', 'ee', ['ul', ['uh-', 'l']]]),\n",
       " ('chews', ['tch', 'oo', 'z']),\n",
       " ('chile', ['tch', ['ih', ['ee']], 'l', 'ee']),\n",
       " ('chilly', ['tch', ['ih', ['ee']], 'l', 'ee']),\n",
       " ('choose', ['tch', 'oo', 'z']),\n",
       " ('chord', ['k', ['aw', ['o']], 'r', 'd']),\n",
       " ('chords', ['k', ['aw', ['o']], 'r', 'd', 'z']),\n",
       " ('chute', ['sh', 'oo', 't']),\n",
       " ('chutes', ['sh', 'oo', 't', 's']),\n",
       " ('coarse', ['k', ['aw', ['o']], 'r', 's']),\n",
       " ('coco', ['k', 'o', 'k', 'o']),\n",
       " ('cocoa', ['k', 'o', 'k', 'o']),\n",
       " ('cord', ['k', ['aw', ['o']], 'r', 'd']),\n",
       " ('cords', ['k', ['aw', ['o']], 'r', 'd', 'z']),\n",
       " ('core', ['k', ['aw', ['o']], 'r']),\n",
       " ('corps', ['k', ['aw', ['o']], 'r']),\n",
       " ('council', ['k', 'au', 'n', 's', ['ul', ['uh-', 'l']]]),\n",
       " ('counsel', ['k', 'au', 'n', 's', ['ul', ['uh-', 'l']]]),\n",
       " ('course', ['k', ['aw', ['o']], 'r', 's']),\n",
       " ('crews', ['k', 'r', 'oo', 'z']),\n",
       " ('cruise', ['k', 'r', 'oo', 'z']),\n",
       " ('dear', ['d', ['ih', ['ee']], 'r']),\n",
       " ('deer', ['d', ['ih', ['ee']], 'r']),\n",
       " ('dew', ['d', 'oo']),\n",
       " ('die', ['d', 'ai']),\n",
       " ('died', ['d', 'ai', 'd']),\n",
       " ('dies', ['d', 'ai', 'z']),\n",
       " ('doc', ['d', 'ah', 'k']),\n",
       " ('dock', ['d', 'ah', 'k']),\n",
       " ('doe', ['d', 'o']),\n",
       " ('dough', ['d', 'o']),\n",
       " ('drier', ['d', 'r', 'ai', ['er', 'r', ['er', 'r']]]),\n",
       " ('dryer', ['d', 'r', 'ai', ['er', 'r', ['er', 'r']]]),\n",
       " ('due', ['d', 'oo']),\n",
       " ('dye', ['d', 'ai']),\n",
       " ('dyed', ['d', 'ai', 'd']),\n",
       " ('dyes', ['d', 'ai', 'z']),\n",
       " ('elicit', ['ih', 'l', ['ih', ['ee']], 's', 'ih', 't']),\n",
       " ('ensure', ['eh', 'n', 'sh', 'u', 'r']),\n",
       " ('fair', ['f', 'eh', 'r']),\n",
       " ('fare', ['f', 'eh', 'r']),\n",
       " ('feat', ['f', 'ee', 't']),\n",
       " ('feet', ['f', 'ee', 't']),\n",
       " ('fiance', ['f', 'ee', 'ah', 'n', 's', 'ay']),\n",
       " ('fiancee', ['f', 'ee', 'ae', 'n', 's', 'ee']),\n",
       " ('fill', ['f', ['ih', ['ee']], 'l']),\n",
       " ('find', ['f', 'ai', 'n', 'd']),\n",
       " ('fined', ['f', 'ai', 'n', 'd']),\n",
       " ('flair', ['f', 'l', 'eh', 'r']),\n",
       " ('flare', ['f', 'l', 'eh', 'r']),\n",
       " ('flea', ['f', 'l', 'ee']),\n",
       " ('flee', ['f', 'l', 'ee']),\n",
       " ('flew', ['f', 'l', 'oo']),\n",
       " ('flour', ['f', 'l', 'au', ['er', 'r', ['er', 'r']]]),\n",
       " ('flower', ['f', 'l', 'au', ['er', 'r', ['er', 'r']]]),\n",
       " ('flowers', ['f', 'l', 'au', ['er', 'r', ['er', 'r']], 'z']),\n",
       " ('flu', ['f', 'l', 'oo']),\n",
       " ('franc', ['f', 'r', 'ae', 'ng', 'k']),\n",
       " ('frank', ['f', 'r', 'ae', 'ng', 'k']),\n",
       " ('grate', ['g', 'r', 'ay', 't']),\n",
       " ('great', ['g', 'r', 'ay', 't']),\n",
       " ('grisly', ['g', 'r', ['ih', ['ee']], 'z', 'l', 'ee']),\n",
       " ('grizzly', ['g', 'r', ['ih', ['ee']], 'z', 'l', 'ee']),\n",
       " ('guessed', ['g', 'eh', 's', 't']),\n",
       " ('guest', ['g', 'eh', 's', 't']),\n",
       " ('guise', ['g', 'ai', 'z']),\n",
       " ('guys', ['g', 'ai', 'z']),\n",
       " ('gym', ['dj', ['ih', ['ee']], 'm']),\n",
       " ('hall', ['h', 'aw', 'l']),\n",
       " ('halls', ['h', 'aw', 'l', 'z']),\n",
       " ('hart', ['h', 'ah', 'r', 't']),\n",
       " ('haul', ['h', 'aw', 'l']),\n",
       " ('hauls', ['h', 'aw', 'l', 'z']),\n",
       " ('heal', ['h', 'ee', 'l']),\n",
       " ('heals', ['h', 'ee', 'l', 'z']),\n",
       " ('hear', ['h', 'ee', 'r']),\n",
       " ('heard', ['h', ['er', 'r', ['er', 'r']], 'd']),\n",
       " ('heart', ['h', 'ah', 'r', 't']),\n",
       " ('heel', ['h', 'ee', 'l']),\n",
       " ('heels', ['h', 'ee', 'l', 'z']),\n",
       " ('heirs', ['eh', 'r', 'z']),\n",
       " ('herd', ['h', ['er', 'r', ['er', 'r']], 'd']),\n",
       " ('here', ['h', 'ee', 'r']),\n",
       " ('hertz', ['h', 'eh', 'r', 't', 's']),\n",
       " ('higher', ['h', 'ai', ['er', 'r', ['er', 'r']]]),\n",
       " ('hire', ['h', 'ai', ['er', 'r', ['er', 'r']]]),\n",
       " ('hoarse', ['h', ['aw', ['o']], 'r', 's']),\n",
       " ('hold', ['h', 'o', 'l', 'd']),\n",
       " ('hole', ['h', 'o', 'l']),\n",
       " ('holed', ['h', 'o', 'l', 'd']),\n",
       " ('holes', ['h', 'o', 'l', 'z']),\n",
       " ('horse', ['h', ['aw', ['o']], 'r', 's']),\n",
       " ('hurts', ['h', ['er', 'r', ['er', 'r']], 't', 's']),\n",
       " ('illicit', ['ih', 'l', ['ih', ['ee']], 's', 'uh-', 't']),\n",
       " ('insure', ['ih', 'n', 'sh', 'u', 'r']),\n",
       " ('jim', ['dj', ['ih', ['ee']], 'm']),\n",
       " ('kneading', ['n', 'ee', 'd', 'ih', 'ng']),\n",
       " ('knew', ['n', 'oo']),\n",
       " ('knight', ['n', 'ai', 't']),\n",
       " ('knights', ['n', 'ai', 't', 's']),\n",
       " ('knit', ['n', ['ih', ['ee']], 't']),\n",
       " ('knows', ['n', 'o', 'z']),\n",
       " ('lacks', ['l', 'ae', ['ks', ['k', 's']]]),\n",
       " ('lax', ['l', 'ae', ['ks', ['k', 's']]]),\n",
       " ('lea', ['l', 'ee']),\n",
       " ('leased', ['l', 'ee', 's', 't']),\n",
       " ('least', ['l', 'ee', 's', 't']),\n",
       " ('lee', ['l', 'ee']),\n",
       " ('lessen', ['l', 'eh', 's', ['un', ['uh-', 'n']]]),\n",
       " ('lesson', ['l', 'eh', 's', ['un', ['uh-', 'n']]]),\n",
       " ('levee', ['l', 'eh', 'v', 'ee']),\n",
       " ('levy', ['l', 'eh', 'v', 'ee']),\n",
       " ('loan', ['l', 'o', 'n']),\n",
       " ('lone', ['l', 'o', 'n']),\n",
       " ('lumbar', ['l', 'uh+', 'm', 'b', 'ah', 'r']),\n",
       " ('lumber', ['l', 'uh+', 'm', 'b', ['er', 'r', ['er', 'r']]]),\n",
       " ('mac', ['m', 'ae', 'k']),\n",
       " ('mach', ['m', 'ah', 'k']),\n",
       " ('made', ['m', 'ay', 'd']),\n",
       " ('maid', ['m', 'ay', 'd']),\n",
       " ('mail', ['m', 'ay', 'l']),\n",
       " ('mails', ['m', 'ay', 'l', 'z']),\n",
       " ('male', ['m', 'ay', 'l']),\n",
       " ('males', ['m', 'ay', 'l', 'z']),\n",
       " ('mall', ['m', 'aw', 'l']),\n",
       " ('manner', ['m', 'ae', 'n', ['er', 'r', ['er', 'r']]]),\n",
       " ('manor', ['m', 'ae', 'n', ['er', 'r', ['er', 'r']]]),\n",
       " ('marks', ['m', 'ah', 'r', ['ks', ['k', 's']]]),\n",
       " ('marx', ['m', 'ah', 'r', ['ks', ['k', 's']]]),\n",
       " ('maul', ['m', 'aw', 'l']),\n",
       " ('meat', ['m', 'ee', 't']),\n",
       " ('meats', ['m', 'ee', 't', 's']),\n",
       " ('meet', ['m', 'ee', 't']),\n",
       " ('meets', ['m', 'ee', 't', 's']),\n",
       " ('missed', ['m', ['ih', ['ee']], 's', 't']),\n",
       " ('mist', ['m', ['ih', ['ee']], 's', 't']),\n",
       " ('mode', ['m', 'o', 'd']),\n",
       " ('moose', ['m', 'oo', 's']),\n",
       " ('morning', ['m', ['aw', ['o']], 'r', 'n', 'ih', 'ng']),\n",
       " ('mourning', ['m', ['aw', ['o']], 'r', 'n', 'ih', 'ng']),\n",
       " ('mowed', ['m', 'o', 'd']),\n",
       " ('naval', ['n', 'ay', 'v', ['ul', ['uh-', 'l']]]),\n",
       " ('needing', ['n', 'ee', 'd', 'ih', 'ng']),\n",
       " ('new', ['n', 'oo']),\n",
       " ('night', ['n', 'ai', 't']),\n",
       " ('nights', ['n', 'ai', 't', 's']),\n",
       " ('nit', ['n', ['ih', ['ee']], 't']),\n",
       " ('nose', ['n', 'o', 'z']),\n",
       " ('paced', ['p', 'ay', 's', 't']),\n",
       " ('packed', ['p', 'ae', 'k', 't']),\n",
       " ('pact', ['p', 'ae', 'k', 't']),\n",
       " ('pain', ['p', 'ay', 'n']),\n",
       " ('pains', ['p', 'ay', 'n', 'z']),\n",
       " ('pair', ['p', 'eh', 'r']),\n",
       " ('pairs', ['p', 'eh', 'r', 'z']),\n",
       " ('pane', ['p', 'ay', 'n']),\n",
       " ('panes', ['p', 'ay', 'n', 'z']),\n",
       " ('passed', ['p', 'ae', 's', 't']),\n",
       " ('past', ['p', 'ae', 's', 't']),\n",
       " ('paste', ['p', 'ay', 's', 't']),\n",
       " ('pause', ['p', 'aw', 'z']),\n",
       " ('paws', ['p', 'aw', 'z']),\n",
       " ('pea', ['p', 'ee']),\n",
       " ('peace', ['p', 'ee', 's']),\n",
       " ('peak', ['p', 'ee', 'k']),\n",
       " ('peaked', ['p', 'ee', 'k', 't']),\n",
       " ('pear', ['p', 'eh', 'r']),\n",
       " ('pee', ['p', 'ee']),\n",
       " ('peek', ['p', 'ee', 'k']),\n",
       " ('peer', ['p', ['ih', ['ee']], 'r']),\n",
       " ('peers', ['p', ['ih', ['ee']], 'r', 'z']),\n",
       " ('phil', ['f', ['ih', ['ee']], 'l']),\n",
       " ('pi', ['p', 'ai']),\n",
       " ('pie', ['p', 'ai']),\n",
       " ('piece', ['p', 'ee', 's']),\n",
       " ('pier', ['p', ['ih', ['ee']], 'r']),\n",
       " ('piers', ['p', ['ih', ['ee']], 'r', 'z']),\n",
       " ('piqued', ['p', 'ee', 'k', 't']),\n",
       " ('plain', ['p', 'l', 'ay', 'n']),\n",
       " ('plains', ['p', 'l', 'ay', 'n', 'z']),\n",
       " ('plane', ['p', 'l', 'ay', 'n']),\n",
       " ('planes', ['p', 'l', 'ay', 'n', 'z']),\n",
       " ('plum', ['p', 'l', 'uh+', 'm']),\n",
       " ('plumb', ['p', 'l', 'uh+', 'm']),\n",
       " ('pole', ['p', 'o', 'l']),\n",
       " ('poles', ['p', 'o', 'l', 'z']),\n",
       " ('poll', ['p', 'o', 'l']),\n",
       " ('polls', ['p', 'o', 'l', 'z']),\n",
       " ('populace', ['p', 'ah', 'p', ['yu', ['y', 'uh-']], 'l', 'uh-', 's']),\n",
       " ('populous', ['p', 'ah', 'p', ['yu', ['y', 'uh-']], 'l', 'uh-', 's']),\n",
       " ('praise', ['p', 'r', 'ay', 'z']),\n",
       " ('pray', ['p', 'r', 'ay']),\n",
       " ('praying', ['p', 'r', 'ay', 'ih', 'ng']),\n",
       " ('prey', ['p', 'r', 'ay']),\n",
       " ('preying', ['p', 'r', 'ay', 'ih', 'ng']),\n",
       " ('preys', ['p', 'r', 'ay', 'z']),\n",
       " ('principal',\n",
       "  ['p', 'r', ['ih', ['ee']], 'n', 's', 'uh-', 'p', ['ul', ['uh-', 'l']]]),\n",
       " ('principle',\n",
       "  ['p', 'r', ['ih', ['ee']], 'n', 's', 'uh-', 'p', ['ul', ['uh-', 'l']]]),\n",
       " ('puts', ['p', 'u', 't', 's']),\n",
       " ('putts', ['p', 'uh+', 't', 's']),\n",
       " ('rack', ['r', 'ae', 'k']),\n",
       " ('rain', ['r', 'ay', 'n']),\n",
       " ('rained', ['r', 'ay', 'n', 'd']),\n",
       " ('rains', ['r', 'ay', 'n', 'z']),\n",
       " ('raise', ['r', 'ay', 'z']),\n",
       " ('rap', ['r', 'ae', 'p']),\n",
       " ('rays', ['r', 'ay', 'z']),\n",
       " ('reeks', ['r', 'ee', ['ks', ['k', 's']]]),\n",
       " ('reigned', ['r', 'ay', 'n', 'd']),\n",
       " ('rein', ['r', 'ay', 'n']),\n",
       " ('reins', ['r', 'ay', 'n', 'z']),\n",
       " ('right', ['r', 'ai', 't']),\n",
       " ('rights', ['r', 'ai', 't', 's']),\n",
       " ('ring', ['r', ['ih', ['ee']], 'ng']),\n",
       " ('ringing', ['r', ['ih', ['ee']], 'ng', 'ih', 'ng']),\n",
       " ('road', ['r', 'o', 'd']),\n",
       " ('roam', ['r', 'o', 'm']),\n",
       " ('rode', ['r', 'o', 'd']),\n",
       " ('roles', ['r', 'o', 'l', 'z']),\n",
       " ('rolls', ['r', 'o', 'l', 'z']),\n",
       " ('rome', ['r', 'o', 'm']),\n",
       " ('rose', ['r', 'o', 'z']),\n",
       " ('rote', ['r', 'o', 't']),\n",
       " ('rough', ['r', 'uh+', 'f']),\n",
       " ('rows', ['r', 'o', 'z']),\n",
       " ('ruff', ['r', 'uh+', 'f']),\n",
       " ('sac', ['s', 'ae', 'k']),\n",
       " ('sack', ['s', 'ae', 'k']),\n",
       " ('sacks', ['s', 'ae', ['ks', ['k', 's']]]),\n",
       " ('sail', ['s', 'ay', 'l']),\n",
       " ('sails', ['s', 'ay', 'l', 'z']),\n",
       " ('sale', ['s', 'ay', 'l']),\n",
       " ('sales', ['s', 'ay', 'l', 'z']),\n",
       " ('sax', ['s', 'ae', ['ks', ['k', 's']]]),\n",
       " ('scene', ['s', 'ee', 'n']),\n",
       " ('scents', ['s', 'eh', 'n', 't', 's']),\n",
       " ('sea', ['s', 'ee']),\n",
       " ('sealing', ['s', 'ee', 'l', 'ih', 'ng']),\n",
       " ('seam', ['s', 'ee', 'm']),\n",
       " ('seams', ['s', 'ee', 'm', 'z']),\n",
       " ('seas', ['s', 'ee', 'z']),\n",
       " ('see', ['s', 'ee']),\n",
       " ('seem', ['s', 'ee', 'm']),\n",
       " ('seems', ['s', 'ee', 'm', 'z']),\n",
       " ('seen', ['s', 'ee', 'n']),\n",
       " ('sees', ['s', 'ee', 'z']),\n",
       " ('sell', ['s', 'eh', 'l']),\n",
       " ('seller', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']]]),\n",
       " ('sellers', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']], 'z']),\n",
       " ('sells', ['s', 'eh', 'l', 'z']),\n",
       " ('sent', ['s', 'eh', 'n', 't']),\n",
       " ('serial', ['s', ['ih', ['ee']], 'r', 'ee', ['ul', ['uh-', 'l']]]),\n",
       " ('shear', ['sh', ['ih', ['ee']], 'r']),\n",
       " ('sheer', ['sh', ['ih', ['ee']], 'r']),\n",
       " ('shoot', ['sh', 'oo', 't']),\n",
       " ('shoots', ['sh', 'oo', 't', 's']),\n",
       " ('sight', ['s', 'ai', 't']),\n",
       " ('sighted', ['s', 'ai', 't', 'uh-', 'd']),\n",
       " ('sights', ['s', 'ai', 't', 's']),\n",
       " ('sink', ['s', ['ih', ['ee']], 'ng', 'k']),\n",
       " ('site', ['s', 'ai', 't']),\n",
       " ('sited', ['s', 'ai', 't', 'ih', 'd']),\n",
       " ('sites', ['s', 'ai', 't', 's']),\n",
       " ('slay', ['s', 'l', 'ay']),\n",
       " ('sleigh', ['s', 'l', 'ay']),\n",
       " ('sole', ['s', 'o', 'l']),\n",
       " ('soles', ['s', 'o', 'l', 'z']),\n",
       " ('son', ['s', 'uh+', 'n']),\n",
       " ('sons', ['s', 'uh+', 'n', 'z']),\n",
       " ('soul', ['s', 'o', 'l']),\n",
       " ('souls', ['s', 'o', 'l', 'z']),\n",
       " ('spade', ['s', 'p', 'ay', 'd']),\n",
       " ('spayed', ['s', 'p', 'ay', 'd']),\n",
       " ('stake', ['s', 't', 'ay', 'k']),\n",
       " ('stakes', ['s', 't', 'ay', ['ks', ['k', 's']]]),\n",
       " ('steak', ['s', 't', 'ay', 'k']),\n",
       " ('steaks', ['s', 't', 'ay', ['ks', ['k', 's']]]),\n",
       " ('steal', ['s', 't', 'ee', 'l']),\n",
       " ('steel', ['s', 't', 'ee', 'l']),\n",
       " ('straight', ['s', 't', 'r', 'ay', 't']),\n",
       " ('strait', ['s', 't', 'r', 'ay', 't']),\n",
       " ('suede', ['s', 'w', 'ay', 'd']),\n",
       " ('suites', ['s', 'w', 'ee', 't', 's']),\n",
       " ('sun', ['s', 'uh+', 'n']),\n",
       " ('suns', ['s', 'uh+', 'n', 'z']),\n",
       " ('swayed', ['s', 'w', 'ay', 'd']),\n",
       " ('sweets', ['s', 'w', 'ee', 't', 's']),\n",
       " ('sync', ['s', ['ih', ['ee']], 'ng', 'k']),\n",
       " ('tacks', ['t', 'ae', ['ks', ['k', 's']]]),\n",
       " ('tail', ['t', 'ay', 'l']),\n",
       " ('tails', ['t', 'ay', 'l', 'z']),\n",
       " ('tale', ['t', 'ay', 'l']),\n",
       " ('tales', ['t', 'ay', 'l', 'z']),\n",
       " ('taught', ['t', 'aw', 't']),\n",
       " ('tax', ['t', 'ae', ['ks', ['k', 's']]]),\n",
       " ('tea', ['t', 'ee']),\n",
       " ('teas', ['t', 'ee', 'z']),\n",
       " ('tease', ['t', 'ee', 'z']),\n",
       " ('tec', ['t', 'eh', 'k']),\n",
       " ('tech', ['t', 'eh', 'k']),\n",
       " ('tee', ['t', 'ee']),\n",
       " ('thai', ['t', 'ai']),\n",
       " ('thais', ['t', 'ai', 'z']),\n",
       " ('throes', ['th-', 'r', 'o', 'z']),\n",
       " ('throws', ['th-', 'r', 'o', 'z']),\n",
       " ('thyme', ['th-', 'ai', 'm']),\n",
       " ('tic', ['t', ['ih', ['ee']], 'k']),\n",
       " ('tick', ['t', ['ih', ['ee']], 'k']),\n",
       " ('tide', ['t', 'ai', 'd']),\n",
       " ('tie', ['t', 'ai']),\n",
       " ('tied', ['t', 'ai', 'd']),\n",
       " ('ties', ['t', 'ai', 'z']),\n",
       " ('time', ['t', 'ai', 'm']),\n",
       " ('toe', ['t', 'o']),\n",
       " ('tow', ['t', 'o']),\n",
       " ('tracked', ['t', 'r', 'ae', 'k', 't']),\n",
       " ('tract', ['t', 'r', 'ae', 'k', 't']),\n",
       " ('wade', ['w', 'ay', 'd']),\n",
       " ('waist', ['w', 'ay', 's', 't']),\n",
       " ('wait', ['w', 'ay', 't']),\n",
       " ('waiting', ['w', 'ay', 't', 'ih', 'ng']),\n",
       " ('waits', ['w', 'ay', 't', 's']),\n",
       " ('waive', ['w', 'ay', 'v']),\n",
       " ('war', ['w', ['aw', ['o']], 'r']),\n",
       " ('warn', ['w', ['aw', ['o']], 'r', 'n']),\n",
       " ('waste', ['w', 'ay', 's', 't']),\n",
       " ('wave', ['w', 'ay', 'v']),\n",
       " ('wax', ['w', 'ae', ['ks', ['k', 's']]]),\n",
       " ('way', ['w', 'ay']),\n",
       " ('ways', ['w', 'ay', 'z']),\n",
       " ('weak', ['w', 'ee', 'k']),\n",
       " ('week', ['w', 'ee', 'k']),\n",
       " ('weigh', ['w', 'ay']),\n",
       " ('weighed', ['w', 'ay', 'd']),\n",
       " ('weighs', ['w', 'ay', 'z']),\n",
       " ('weight', ['w', 'ay', 't']),\n",
       " ('weighting', ['w', 'ay', 't', 'ih', 'ng']),\n",
       " ('weights', ['w', 'ay', 't', 's']),\n",
       " ('whacks', ['w', 'ae', ['ks', ['k', 's']]]),\n",
       " ('whine', ['w', 'ai', 'n']),\n",
       " ('whit', ['w', ['ih', ['ee']], 't']),\n",
       " ('whole', ['h', 'o', 'l']),\n",
       " ('wine', ['w', 'ai', 'n']),\n",
       " ('wit', ['w', ['ih', ['ee']], 't']),\n",
       " ('wore', ['w', ['aw', ['o']], 'r']),\n",
       " ('worn', ['w', ['aw', ['o']], 'r', 'n']),\n",
       " ('wrack', ['r', 'ae', 'k']),\n",
       " ('wrap', ['r', 'ae', 'p']),\n",
       " ('wreaks', ['r', 'ee', ['ks', ['k', 's']]]),\n",
       " ('wring', ['r', ['ih', ['ee']], 'ng']),\n",
       " ('wringing', ['r', ['ih', ['ee']], 'ng', 'ih', 'ng']),\n",
       " ('write', ['r', 'ai', 't']),\n",
       " ('writes', ['r', 'ai', 't', 's']),\n",
       " ('wrote', ['r', 'o', 't'])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom_kcp_word_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get possible (valid) Grapheme strings and probs for each KCP encoded homophone \n",
    "Note: 6 homophones not captured: \n",
    "- corps ['k', ['aw', ['o']], 'r'] -> Silent \"PS\"\n",
    "- guessed: ['g', 'eh', 's', 't’] —> Grapheme 'GUE' as kcp 'g' but no mapping for Grapheme 'GU' as 'g' (silent U) \n",
    "- guest: ['g', 'eh', 's', ’t']\n",
    "- guise: ['g', 'ai', 'z‘]\n",
    "- thai: ['t', 'ai‘] —> KCP Symbol 'ai' not mapped to Grapheme 'AI' in Berndt'sconditional probs\n",
    "- thais: ['t', 'ai', 'z']\n",
    "- weighed: ['w', 'ay', 'd‘] —> Grapheme 'EIGH' as kcp 'ay' but not EIGH-E (silent E)\n",
    "\n",
    "Possible Solution: \n",
    "- corps ['k', ['aw', ['o']], 'r'] add a 's' to the KCP encoding to capture the \"PS\" with conditional probability 1. This has the least influence on the probabilites and is comparable to the to the case of a silent 'H' in the beginning of a word. \n",
    "- guessed: ['g', 'eh', 's', 't’] add an 'e' --> 'gueessed' in order to get the 'eh' to 'E' mapping after the silent u \n",
    "- guest: ['g', 'eh', 's', 't’] \n",
    "- guise: ['g', 'ai', 'z‘] add an 'e' --> 'gueise' in order to get the hard 'g' for the 'G' with silent 'U'\n",
    "- thai : ['t', 'ai‘] add an 'e' at the end because 'AI-E' has a mapping for 'ai' \n",
    "- thais: \n",
    "- weighed: ['w', 'ay', 'd‘] --> drop the silent 'e' to get 'weighd' ['w', 'ay', 'd‘]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_grapheme_strings, possible_prior_probs, possible_cond_probs, word_rests = word_pronunciation_predictibility.get_grapheme_string_with_conditional_prob_for_keyboard_phonetics(hom_kcp_word_tuples, \n",
    "phonem_graphem_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corps ['k', ['aw', ['o']], 'r']\n",
      "guessed ['g', 'eh', 's', 't']\n",
      "guest ['g', 'eh', 's', 't']\n",
      "guise ['g', 'ai', 'z']\n",
      "thai ['t', 'ai']\n",
      "thais ['t', 'ai', 'z']\n",
      "weighed ['w', 'ay', 'd']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# homophones for which we have no valid grapheme string: \n",
    "counter = 0 \n",
    "for i,word_pron in enumerate(hom_kcp_word_tuples):\n",
    "    word = word_pron[0] # word string\n",
    "    pron = word_pron[1] # list of keyboard compatible phon characters\n",
    "    if len(possible_grapheme_strings[i]) == 0 :\n",
    "        counter+=1\n",
    "        print(word,pron)\n",
    "    else:\n",
    "        empty_string = ['' != i for i in word_rests[i]]\n",
    "        if np.sum(empty_string) == len(word_rests[i]):\n",
    "            counter+=1\n",
    "            print(word,pron)\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_hom_kcp_word_tuples = [\n",
    "    ('corps', ['k', ['aw', ['o']], 'r', 's']),\n",
    "    ('gueessed',['g', 'eh', 's', 't']),\n",
    "    ('gueest', ['g', 'eh', 's', 't']),\n",
    "    ('gueise', ['g', 'ai', 'z']),\n",
    "    ('thaie', ['t', 'ai']),\n",
    "    ('thaies', ['t', 'ai', 'z']),\n",
    "    ('weighd', ['w', 'ay', 'd'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_hom_kcp_word_tuples_dict = {\n",
    "    'corps': ('corps', ['k', ['aw', ['o']], 'r', 's']),\n",
    "    'guessed' : ('gueessed',['g', 'eh', 's', 't']),\n",
    "    'guest': ('gueest', ['g', 'eh', 's', 't']),\n",
    "    'guise': ('gueise', ['g', 'ai', 'z']),\n",
    "    'thai' :('thaie', ['t', 'ai']),\n",
    "    'thais':('thaies', ['t', 'ai', 'z']),\n",
    "    'weighed':('weighd', ['w', 'ay', 'd'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_grapheme_strings_problematic_homs, possible_prior_probs_problematic_homs, possible_cond_probs_problematic_homs, word_rests_problematic_homs = word_pronunciation_predictibility.get_grapheme_string_with_conditional_prob_for_keyboard_phonetics(problematic_hom_kcp_word_tuples, \n",
    "phonem_graphem_prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i,wp in enumerate(hom_kcp_word_tuples):\n",
    "    word = wp[0]\n",
    "    phon = wp[1]\n",
    "    if word in problematic_hom_kcp_word_tuples_dict:\n",
    "        possible_grapheme_strings[i] = possible_grapheme_strings_problematic_homs[j]\n",
    "        word_rests[i] = word_rests_problematic_homs[j]\n",
    "        possible_prior_probs[i] = possible_prior_probs_problematic_homs[j]\n",
    "        possible_cond_probs[i] = possible_cond_probs_problematic_homs[j]\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_word_rests,valid_grapheme_strings,valid_prior_probs, valid_cond_probs = word_pronunciation_predictibility.get_valid_grapheme_strings(hom_kcp_word_tuples, possible_grapheme_strings, word_rests,possible_prior_probs, possible_cond_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# homophones for which we have no valid grapheme string: \n",
    "counter = 0 \n",
    "for i,word_pron in enumerate(hom_kcp_word_tuples):\n",
    "    word = word_pron[0] # word string\n",
    "    pron = word_pron[1] # list of keyboard compatible phon characters\n",
    "    if len(valid_grapheme_strings[i]) == 0:\n",
    "        counter+=1\n",
    "        print(word,pron)        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aides', ['ay', 'd', 'z'])\n",
      "[[0.734 0.991 1.   ]\n",
      " [0.818 0.991 0.12 ]]\n",
      "[['AI' 'D' 'ES']\n",
      " ['AI-E' 'D' 'S']] \n",
      "\n",
      "('allowed', [['ul', ['uh-', 'l']], 'au', 'd'])\n",
      "[list([1.0, 1.0, 0.6659999999999999, 0.991])\n",
      " list([0.18600000000000003, 1.0, 1.0, 0.6659999999999999, 0.991])\n",
      " list([0.18600000000000003, 1.0, 0.6659999999999999, 0.991])]\n",
      "[list(['AL', 'L', 'OW-E', 'D']) list(['A', 'L', 'L', 'OW-E', 'D'])\n",
      " list(['A', 'LL', 'OW-E', 'D'])] \n",
      "\n",
      "('aloud', [['ul', ['uh-', 'l']], 'au', 'd'])\n",
      "[list([1.0, 0.324, 0.991]) list([0.18600000000000003, 1.0, 0.324, 0.991])]\n",
      "[list(['AL', 'OU', 'D']) list(['A', 'L', 'OU', 'D'])] \n",
      "\n",
      "('beets', ['b', 'ee', 't', 's'])\n",
      "[[1.    0.252 0.973 0.868]\n",
      " [1.    0.979 0.973 0.868]]\n",
      "[['B' 'E-E' 'T' 'S']\n",
      " ['B' 'EE' 'T' 'S']] \n",
      "\n",
      "('billed', ['b', ['ih', ['ee']], 'l', 'd'])\n",
      "[[1.    0.356 1.    0.991]\n",
      " [1.    0.046 1.    0.991]\n",
      " [1.    0.356 1.    1.   ]\n",
      " [1.    0.046 1.    1.   ]]\n",
      "[['B' 'I-E' 'LL' 'D']\n",
      " ['B' 'I-E' 'LL' 'D']\n",
      " ['B' 'I-E' 'L' 'LD']\n",
      " ['B' 'I-E' 'L' 'LD']] \n",
      "\n",
      "('blue', ['b', 'l', 'oo'])\n",
      "[[1.    1.    0.093]\n",
      " [1.    1.    0.372]]\n",
      "[['B' 'L' 'U-E']\n",
      " ['B' 'L' 'UE']] \n",
      "\n",
      "('boar', ['b', ['aw', ['o']], 'r'])\n",
      "[[1.    0.066 1.   ]\n",
      " [1.    0.933 1.   ]]\n",
      "[['B' 'OA' 'R']\n",
      " ['B' 'OA' 'R']] \n",
      "\n",
      "('board', ['b', ['aw', ['o']], 'r', 'd'])\n",
      "[[1.    0.066 1.    0.991]\n",
      " [1.    0.933 1.    0.991]]\n",
      "[['B' 'OA' 'R' 'D']\n",
      " ['B' 'OA' 'R' 'D']] \n",
      "\n",
      "('bore', ['b', ['aw', ['o']], 'r'])\n",
      "[[1.    0.044 1.   ]\n",
      " [1.    0.785 1.   ]]\n",
      "[['B' 'O-E' 'R']\n",
      " ['B' 'O-E' 'R']] \n",
      "\n",
      "('bored', ['b', ['aw', ['o']], 'r', 'd'])\n",
      "[[1.    0.044 1.    0.991]\n",
      " [1.    0.785 1.    0.991]]\n",
      "[['B' 'O-E' 'R' 'D']\n",
      " ['B' 'O-E' 'R' 'D']] \n",
      "\n",
      "('capital', ['k', 'ae', 'p', 'uh-', 't', ['ul', ['uh-', 'l']]])\n",
      "[list([0.757, 0.542, 1.0, 0.18, 0.973, 1.0])\n",
      " list([0.757, 0.542, 1.0, 0.18, 0.973, 0.18600000000000003, 1.0])]\n",
      "[list(['C', 'A', 'P', 'I', 'T', 'AL'])\n",
      " list(['C', 'A', 'P', 'I', 'T', 'A', 'L'])] \n",
      "\n",
      "('capitol', ['k', 'ae', 'p', 'ih', 't', ['ul', ['uh-', 'l']]])\n",
      "[list([0.757, 0.542, 1.0, 0.716, 0.973, 1.0])\n",
      " list([0.757, 0.542, 1.0, 0.716, 0.973, 0.26899999999999996, 1.0])]\n",
      "[list(['C', 'A', 'P', 'I', 'T', 'OL'])\n",
      " list(['C', 'A', 'P', 'I', 'T', 'O', 'L'])] \n",
      "\n",
      "('cellar', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.23399999999999999, 0.419, 1.0, 0.021, 1.0])\n",
      " list([0.23399999999999999, 0.419, 1.0, 0.021, 1.0])]\n",
      "[list(['C', 'E', 'LL', 'A', 'R']) list(['C', 'E', 'LL', 'A', 'R'])] \n",
      "\n",
      "('cellars', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']], 'z'])\n",
      "[[0.234 0.419 1.    0.021 1.    0.12 ]\n",
      " [0.234 0.419 1.    0.021 1.    0.12 ]]\n",
      "[['C' 'E' 'LL' 'A' 'R' 'S']\n",
      " ['C' 'E' 'LL' 'A' 'R' 'S']] \n",
      "\n",
      "('cereal', ['s', ['ih', ['ee']], 'r', 'ee', ['ul', ['uh-', 'l']]])\n",
      "[list([0.23399999999999999, 0.0006, 1.0, 0.23, 1.0])\n",
      " list([0.23399999999999999, 0.23, 1.0, 0.23, 1.0])\n",
      " list([0.23399999999999999, 0.0006, 1.0, 0.23, 0.18600000000000003, 1.0])\n",
      " list([0.23399999999999999, 0.23, 1.0, 0.23, 0.18600000000000003, 1.0])]\n",
      "[list(['C', 'E', 'R', 'E', 'AL']) list(['C', 'E', 'R', 'E', 'AL'])\n",
      " list(['C', 'E', 'R', 'E', 'A', 'L']) list(['C', 'E', 'R', 'E', 'A', 'L'])] \n",
      "\n",
      "('chile', ['tch', ['ih', ['ee']], 'l', 'ee'])\n",
      "[[0.64  0.716 1.    0.23 ]\n",
      " [0.64  0.005 1.    0.23 ]]\n",
      "[['CH' 'I' 'L' 'E']\n",
      " ['CH' 'I' 'L' 'E']] \n",
      "\n",
      "('chilly', ['tch', ['ih', ['ee']], 'l', 'ee'])\n",
      "[[0.64  0.716 1.    0.786]\n",
      " [0.64  0.005 1.    0.786]]\n",
      "[['CH' 'I' 'LL' 'Y']\n",
      " ['CH' 'I' 'LL' 'Y']] \n",
      "\n",
      "('chord', ['k', ['aw', ['o']], 'r', 'd'])\n",
      "[[0.29  0.072 1.    0.991]\n",
      " [0.29  0.314 1.    0.991]]\n",
      "[['CH' 'O' 'R' 'D']\n",
      " ['CH' 'O' 'R' 'D']] \n",
      "\n",
      "('chords', ['k', ['aw', ['o']], 'r', 'd', 'z'])\n",
      "[[0.29  0.072 1.    0.991 0.12 ]\n",
      " [0.29  0.314 1.    0.991 0.12 ]]\n",
      "[['CH' 'O' 'R' 'D' 'S']\n",
      " ['CH' 'O' 'R' 'D' 'S']] \n",
      "\n",
      "('cord', ['k', ['aw', ['o']], 'r', 'd'])\n",
      "[[0.757 0.072 1.    0.991]\n",
      " [0.757 0.314 1.    0.991]]\n",
      "[['C' 'O' 'R' 'D']\n",
      " ['C' 'O' 'R' 'D']] \n",
      "\n",
      "('cords', ['k', ['aw', ['o']], 'r', 'd', 'z'])\n",
      "[[0.757 0.072 1.    0.991 0.12 ]\n",
      " [0.757 0.314 1.    0.991 0.12 ]]\n",
      "[['C' 'O' 'R' 'D' 'S']\n",
      " ['C' 'O' 'R' 'D' 'S']] \n",
      "\n",
      "('core', ['k', ['aw', ['o']], 'r'])\n",
      "[[0.757 0.044 1.   ]\n",
      " [0.757 0.785 1.   ]]\n",
      "[['C' 'O-E' 'R']\n",
      " ['C' 'O-E' 'R']] \n",
      "\n",
      "('council', ['k', 'au', 'n', 's', ['ul', ['uh-', 'l']]])\n",
      "[list([0.757, 0.324, 0.9670000000000001, 0.23399999999999999, 1.0])\n",
      " list([0.757, 0.324, 0.9670000000000001, 0.23399999999999999, 0.18, 1.0])]\n",
      "[list(['C', 'OU', 'N', 'C', 'IL']) list(['C', 'OU', 'N', 'C', 'I', 'L'])] \n",
      "\n",
      "('counsel', ['k', 'au', 'n', 's', ['ul', ['uh-', 'l']]])\n",
      "[list([0.757, 0.324, 0.9670000000000001, 0.868, 1.0])\n",
      " list([0.757, 0.324, 0.9670000000000001, 0.868, 0.096, 1.0])]\n",
      "[list(['C', 'OU', 'N', 'S', 'EL']) list(['C', 'OU', 'N', 'S', 'E', 'L'])] \n",
      "\n",
      "('deer', ['d', ['ih', ['ee']], 'r'])\n",
      "[[0.991 0.002 1.   ]\n",
      " [0.991 0.02  1.   ]\n",
      " [0.991 0.252 1.   ]\n",
      " [0.991 0.979 1.   ]]\n",
      "[['D' 'E-E' 'R']\n",
      " ['D' 'EE' 'R']\n",
      " ['D' 'E-E' 'R']\n",
      " ['D' 'EE' 'R']] \n",
      "\n",
      "('die', ['d', 'ai'])\n",
      "[[0.991 0.589]\n",
      " [0.991 0.203]]\n",
      "[['D' 'I-E']\n",
      " ['D' 'IE']] \n",
      "\n",
      "('died', ['d', 'ai', 'd'])\n",
      "[[0.991 0.589 0.991]\n",
      " [0.991 0.203 0.991]]\n",
      "[['D' 'I-E' 'D']\n",
      " ['D' 'IE' 'D']] \n",
      "\n",
      "('dies', ['d', 'ai', 'z'])\n",
      "[[0.991 0.074 1.   ]\n",
      " [0.991 0.589 0.12 ]\n",
      " [0.991 0.203 0.12 ]]\n",
      "[['D' 'I' 'ES']\n",
      " ['D' 'I-E' 'S']\n",
      " ['D' 'IE' 'S']] \n",
      "\n",
      "('doe', ['d', 'o'])\n",
      "[[0.991 0.785]\n",
      " [0.991 0.59 ]]\n",
      "[['D' 'O-E']\n",
      " ['D' 'OE']] \n",
      "\n",
      "('drier', ['d', 'r', 'ai', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.991, 1.0, 0.589, 1.0]) list([0.991, 1.0, 0.203, 1.0])\n",
      " list([0.991, 1.0, 0.07400000000000001, 0.249, 1.0])\n",
      " list([0.991, 1.0, 0.07400000000000001, 0.249, 1.0])]\n",
      "[list(['D', 'R', 'I-E', 'R']) list(['D', 'R', 'IE', 'R'])\n",
      " list(['D', 'R', 'I', 'E', 'R']) list(['D', 'R', 'I', 'E', 'R'])] \n",
      "\n",
      "('dryer', ['d', 'r', 'ai', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.991, 1.0, 0.958, 1.0]) list([0.991, 1.0, 0.1, 0.249, 1.0])\n",
      " list([0.991, 1.0, 0.1, 0.249, 1.0])]\n",
      "[list(['D', 'R', 'Y-E', 'R']) list(['D', 'R', 'Y', 'E', 'R'])\n",
      " list(['D', 'R', 'Y', 'E', 'R'])] \n",
      "\n",
      "('due', ['d', 'oo'])\n",
      "[[0.991 0.093]\n",
      " [0.991 0.372]]\n",
      "[['D' 'U-E']\n",
      " ['D' 'UE']] \n",
      "\n",
      "('dyes', ['d', 'ai', 'z'])\n",
      "[[0.991 0.1   1.   ]\n",
      " [0.991 0.958 0.12 ]]\n",
      "[['D' 'Y' 'ES']\n",
      " ['D' 'Y-E' 'S']] \n",
      "\n",
      "('elicit', ['ih', 'l', ['ih', ['ee']], 's', 'ih', 't'])\n",
      "[[6.00e-04 1.00e+00 7.16e-01 2.34e-01 7.16e-01 9.73e-01]\n",
      " [6.00e-04 1.00e+00 5.00e-03 2.34e-01 7.16e-01 9.73e-01]]\n",
      "[['E' 'L' 'I' 'C' 'I' 'T']\n",
      " ['E' 'L' 'I' 'C' 'I' 'T']] \n",
      "\n",
      "('ensure', ['eh', 'n', 'sh', 'u', 'r'])\n",
      "[[0.321 0.967 0.003 0.068 1.   ]\n",
      " [0.419 0.967 0.003 0.03  1.   ]]\n",
      "[['E-E' 'N' 'S' 'U' 'R']\n",
      " ['E' 'N' 'S' 'U-E' 'R']] \n",
      "\n",
      "('feet', ['f', 'ee', 't'])\n",
      "[[0.998 0.252 0.973]\n",
      " [0.998 0.979 0.973]]\n",
      "[['F' 'E-E' 'T']\n",
      " ['F' 'EE' 'T']] \n",
      "\n",
      "('fiancee', ['f', 'ee', 'ae', 'n', 's', 'ee'])\n",
      "[[0.998 0.046 0.542 0.967 0.234 0.23 ]\n",
      " [0.998 0.005 0.121 0.967 0.234 0.23 ]\n",
      " [0.998 0.005 0.542 0.967 0.234 0.252]\n",
      " [0.998 0.005 0.542 0.967 0.234 0.979]]\n",
      "[['F' 'I-E' 'A' 'N' 'C' 'E']\n",
      " ['F' 'I' 'A-E' 'N' 'C' 'E']\n",
      " ['F' 'I' 'A' 'N' 'C' 'E-E']\n",
      " ['F' 'I' 'A' 'N' 'C' 'EE']] \n",
      "\n",
      "('fill', ['f', ['ih', ['ee']], 'l'])\n",
      "[[0.998 0.716 1.   ]\n",
      " [0.998 0.005 1.   ]]\n",
      "[['F' 'I' 'LL']\n",
      " ['F' 'I' 'LL']] \n",
      "\n",
      "('flee', ['f', 'l', 'ee'])\n",
      "[[0.998 1.    0.252]\n",
      " [0.998 1.    0.979]]\n",
      "[['F' 'L' 'E-E']\n",
      " ['F' 'L' 'EE']] \n",
      "\n",
      "('grisly', ['g', 'r', ['ih', ['ee']], 'z', 'l', 'ee'])\n",
      "[[0.64  1.    0.716 0.12  1.    0.786]\n",
      " [0.64  1.    0.005 0.12  1.    0.786]]\n",
      "[['G' 'R' 'I' 'S' 'L' 'Y']\n",
      " ['G' 'R' 'I' 'S' 'L' 'Y']] \n",
      "\n",
      "('grizzly', ['g', 'r', ['ih', ['ee']], 'z', 'l', 'ee'])\n",
      "[[0.64  1.    0.716 1.    1.    0.786]\n",
      " [0.64  1.    0.005 1.    1.    0.786]]\n",
      "[['G' 'R' 'I' 'ZZ' 'L' 'Y']\n",
      " ['G' 'R' 'I' 'ZZ' 'L' 'Y']] \n",
      "\n",
      "('gym', ['dj', ['ih', ['ee']], 'm'])\n",
      "[[0.351 0.073 0.971]\n",
      " [0.351 0.786 0.971]]\n",
      "[['G' 'Y' 'M']\n",
      " ['G' 'Y' 'M']] \n",
      "\n",
      "('heard', ['h', ['er', 'r', ['er', 'r']], 'd'])\n",
      "[[1.    0.056 1.    0.991]\n",
      " [1.    0.056 1.    0.991]]\n",
      "[['H' 'EA' 'R' 'D']\n",
      " ['H' 'EA' 'R' 'D']] \n",
      "\n",
      "('heel', ['h', 'ee', 'l'])\n",
      "[[1.    0.252 1.   ]\n",
      " [1.    0.979 1.   ]]\n",
      "[['H' 'E-E' 'L']\n",
      " ['H' 'EE' 'L']] \n",
      "\n",
      "('heels', ['h', 'ee', 'l', 'z'])\n",
      "[[1.    0.252 1.    0.12 ]\n",
      " [1.    0.979 1.    0.12 ]]\n",
      "[['H' 'E-E' 'L' 'S']\n",
      " ['H' 'EE' 'L' 'S']] \n",
      "\n",
      "('herd', ['h', ['er', 'r', ['er', 'r']], 'd'])\n",
      "[[1.    0.249 1.    0.991]\n",
      " [1.    0.249 1.    0.991]]\n",
      "[['H' 'E' 'R' 'D']\n",
      " ['H' 'E' 'R' 'D']] \n",
      "\n",
      "('higher', ['h', 'ai', ['er', 'r', ['er', 'r']]])\n",
      "[list([1.0, 1.0, 0.249, 1.0]) list([1.0, 1.0, 0.249, 1.0])]\n",
      "[list(['H', 'IGH', 'E', 'R']) list(['H', 'IGH', 'E', 'R'])] \n",
      "\n",
      "('holes', ['h', 'o', 'l', 'z'])\n",
      "[[1.    0.314 1.    1.   ]\n",
      " [1.    0.785 1.    0.12 ]]\n",
      "[['H' 'O' 'L' 'ES']\n",
      " ['H' 'O-E' 'L' 'S']] \n",
      "\n",
      "('horse', ['h', ['aw', ['o']], 'r', 's'])\n",
      "[[1.    0.044 1.    0.868]\n",
      " [1.    0.785 1.    0.868]]\n",
      "[['H' 'O-E' 'R' 'S']\n",
      " ['H' 'O-E' 'R' 'S']] \n",
      "\n",
      "('hurts', ['h', ['er', 'r', ['er', 'r']], 't', 's'])\n",
      "[[1.    0.08  1.    0.973 0.868]\n",
      " [1.    0.08  1.    0.973 0.868]]\n",
      "[['H' 'U' 'R' 'T' 'S']\n",
      " ['H' 'U' 'R' 'T' 'S']] \n",
      "\n",
      "('illicit', ['ih', 'l', ['ih', ['ee']], 's', 'uh-', 't'])\n",
      "[[0.716 1.    0.716 0.234 0.18  0.973]\n",
      " [0.716 1.    0.005 0.234 0.18  0.973]]\n",
      "[['I' 'LL' 'I' 'C' 'I' 'T']\n",
      " ['I' 'LL' 'I' 'C' 'I' 'T']] \n",
      "\n",
      "('insure', ['ih', 'n', 'sh', 'u', 'r'])\n",
      "[[0.356 0.967 0.003 0.068 1.   ]\n",
      " [0.716 0.967 0.003 0.03  1.   ]]\n",
      "[['I-E' 'N' 'S' 'U' 'R']\n",
      " ['I' 'N' 'S' 'U-E' 'R']] \n",
      "\n",
      "('jim', ['dj', ['ih', ['ee']], 'm'])\n",
      "[[1.    0.716 0.971]\n",
      " [1.    0.005 0.971]]\n",
      "[['J' 'I' 'M']\n",
      " ['J' 'I' 'M']] \n",
      "\n",
      "('knit', ['n', ['ih', ['ee']], 't'])\n",
      "[[1.    0.716 0.973]\n",
      " [1.    0.005 0.973]]\n",
      "[['KN' 'I' 'T']\n",
      " ['KN' 'I' 'T']] \n",
      "\n",
      "('lee', ['l', 'ee'])\n",
      "[[1.    0.252]\n",
      " [1.    0.979]]\n",
      "[['L' 'E-E']\n",
      " ['L' 'EE']] \n",
      "\n",
      "('lessen', ['l', 'eh', 's', ['un', ['uh-', 'n']]])\n",
      "[list([1.0, 0.419, 0.9520000000000001, 1.0])\n",
      " list([1.0, 0.419, 0.9520000000000001, 0.096, 0.9670000000000001])]\n",
      "[list(['L', 'E', 'SS', 'EN']) list(['L', 'E', 'SS', 'E', 'N'])] \n",
      "\n",
      "('lesson', ['l', 'eh', 's', ['un', ['uh-', 'n']]])\n",
      "[list([1.0, 0.419, 0.9520000000000001, 1.0])\n",
      " list([1.0, 0.419, 0.9520000000000001, 0.26899999999999996, 0.9670000000000001])]\n",
      "[list(['L', 'E', 'SS', 'ON']) list(['L', 'E', 'SS', 'O', 'N'])] \n",
      "\n",
      "('levee', ['l', 'eh', 'v', 'ee'])\n",
      "[[1.    0.321 1.    0.23 ]\n",
      " [1.    0.419 1.    0.252]\n",
      " [1.    0.419 1.    0.979]]\n",
      "[['L' 'E-E' 'V' 'E']\n",
      " ['L' 'E' 'V' 'E-E']\n",
      " ['L' 'E' 'V' 'EE']] \n",
      "\n",
      "('lumber', ['l', 'uh+', 'm', 'b', ['er', 'r', ['er', 'r']]])\n",
      "[list([1.0, 0.063, 0.971, 1.0, 1.0])\n",
      " list([1.0, 0.41700000000000004, 0.971, 1.0, 0.249, 1.0])\n",
      " list([1.0, 0.41700000000000004, 0.971, 1.0, 0.249, 1.0])]\n",
      "[list(['L', 'U-E', 'M', 'B', 'R']) list(['L', 'U', 'M', 'B', 'E', 'R'])\n",
      " list(['L', 'U', 'M', 'B', 'E', 'R'])] \n",
      "\n",
      "('males', ['m', 'ay', 'l', 'z'])\n",
      "[[0.971 0.129 1.    1.   ]\n",
      " [0.971 0.651 1.    0.12 ]]\n",
      "[['M' 'A' 'L' 'ES']\n",
      " ['M' 'A-E' 'L' 'S']] \n",
      "\n",
      "('manner', ['m', 'ae', 'n', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.971, 0.121, 1.0, 1.0]) list([0.971, 0.542, 1.0, 0.249, 1.0])\n",
      " list([0.971, 0.542, 1.0, 0.249, 1.0])]\n",
      "[list(['M', 'A-E', 'NN', 'R']) list(['M', 'A', 'NN', 'E', 'R'])\n",
      " list(['M', 'A', 'NN', 'E', 'R'])] \n",
      "\n",
      "('manor', ['m', 'ae', 'n', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.971, 0.542, 0.9670000000000001, 0.053, 1.0])\n",
      " list([0.971, 0.542, 0.9670000000000001, 0.053, 1.0])]\n",
      "[list(['M', 'A', 'N', 'O', 'R']) list(['M', 'A', 'N', 'O', 'R'])] \n",
      "\n",
      "('meet', ['m', 'ee', 't'])\n",
      "[[0.971 0.252 0.973]\n",
      " [0.971 0.979 0.973]]\n",
      "[['M' 'E-E' 'T']\n",
      " ['M' 'EE' 'T']] \n",
      "\n",
      "('meets', ['m', 'ee', 't', 's'])\n",
      "[[0.971 0.252 0.973 0.868]\n",
      " [0.971 0.979 0.973 0.868]]\n",
      "[['M' 'E-E' 'T' 'S']\n",
      " ['M' 'EE' 'T' 'S']] \n",
      "\n",
      "('missed', ['m', ['ih', ['ee']], 's', 't'])\n",
      "[[0.971 0.716 0.952 1.   ]\n",
      " [0.971 0.005 0.952 1.   ]]\n",
      "[['M' 'I' 'SS' 'ED']\n",
      " ['M' 'I' 'SS' 'ED']] \n",
      "\n",
      "('mist', ['m', ['ih', ['ee']], 's', 't'])\n",
      "[[0.971 0.716 0.868 0.973]\n",
      " [0.971 0.005 0.868 0.973]]\n",
      "[['M' 'I' 'S' 'T']\n",
      " ['M' 'I' 'S' 'T']] \n",
      "\n",
      "('morning', ['m', ['aw', ['o']], 'r', 'n', 'ih', 'ng'])\n",
      "[[0.971 0.072 1.    0.967 0.716 1.   ]\n",
      " [0.971 0.314 1.    0.967 0.716 1.   ]]\n",
      "[['M' 'O' 'R' 'N' 'I' 'NG']\n",
      " ['M' 'O' 'R' 'N' 'I' 'NG']] \n",
      "\n",
      "('naval', ['n', 'ay', 'v', ['ul', ['uh-', 'l']]])\n",
      "[list([0.9670000000000001, 0.129, 1.0, 1.0])\n",
      " list([0.9670000000000001, 0.129, 1.0, 0.18600000000000003, 1.0])]\n",
      "[list(['N', 'A', 'V', 'AL']) list(['N', 'A', 'V', 'A', 'L'])] \n",
      "\n",
      "('needing', ['n', 'ee', 'd', 'ih', 'ng'])\n",
      "[[0.967 0.252 0.991 0.716 1.   ]\n",
      " [0.967 0.979 0.991 0.716 1.   ]]\n",
      "[['N' 'E-E' 'D' 'I' 'NG']\n",
      " ['N' 'EE' 'D' 'I' 'NG']] \n",
      "\n",
      "('nit', ['n', ['ih', ['ee']], 't'])\n",
      "[[0.967 0.716 0.973]\n",
      " [0.967 0.005 0.973]]\n",
      "[['N' 'I' 'T']\n",
      " ['N' 'I' 'T']] \n",
      "\n",
      "('panes', ['p', 'ay', 'n', 'z'])\n",
      "[[1.    0.129 0.967 1.   ]\n",
      " [1.    0.651 0.967 0.12 ]]\n",
      "[['P' 'A' 'N' 'ES']\n",
      " ['P' 'A-E' 'N' 'S']] \n",
      "\n",
      "('pee', ['p', 'ee'])\n",
      "[[1.    0.252]\n",
      " [1.    0.979]]\n",
      "[['P' 'E-E']\n",
      " ['P' 'EE']] \n",
      "\n",
      "('peek', ['p', 'ee', 'k'])\n",
      "[[1.    0.252 1.   ]\n",
      " [1.    0.979 1.   ]]\n",
      "[['P' 'E-E' 'K']\n",
      " ['P' 'EE' 'K']] \n",
      "\n",
      "('peer', ['p', ['ih', ['ee']], 'r'])\n",
      "[[1.    0.002 1.   ]\n",
      " [1.    0.02  1.   ]\n",
      " [1.    0.252 1.   ]\n",
      " [1.    0.979 1.   ]]\n",
      "[['P' 'E-E' 'R']\n",
      " ['P' 'EE' 'R']\n",
      " ['P' 'E-E' 'R']\n",
      " ['P' 'EE' 'R']] \n",
      "\n",
      "('peers', ['p', ['ih', ['ee']], 'r', 'z'])\n",
      "[[1.    0.002 1.    0.12 ]\n",
      " [1.    0.02  1.    0.12 ]\n",
      " [1.    0.252 1.    0.12 ]\n",
      " [1.    0.979 1.    0.12 ]]\n",
      "[['P' 'E-E' 'R' 'S']\n",
      " ['P' 'EE' 'R' 'S']\n",
      " ['P' 'E-E' 'R' 'S']\n",
      " ['P' 'EE' 'R' 'S']] \n",
      "\n",
      "('phil', ['f', ['ih', ['ee']], 'l'])\n",
      "[[1.    0.716 1.   ]\n",
      " [1.    0.005 1.   ]]\n",
      "[['PH' 'I' 'L']\n",
      " ['PH' 'I' 'L']] \n",
      "\n",
      "('pie', ['p', 'ai'])\n",
      "[[1.    0.589]\n",
      " [1.    0.203]]\n",
      "[['P' 'I-E']\n",
      " ['P' 'IE']] \n",
      "\n",
      "('pier', ['p', ['ih', ['ee']], 'r'])\n",
      "[[1.    0.356 1.   ]\n",
      " [1.    0.101 1.   ]\n",
      " [1.    0.046 1.   ]\n",
      " [1.    0.492 1.   ]]\n",
      "[['P' 'I-E' 'R']\n",
      " ['P' 'IE' 'R']\n",
      " ['P' 'I-E' 'R']\n",
      " ['P' 'IE' 'R']] \n",
      "\n",
      "('piers', ['p', ['ih', ['ee']], 'r', 'z'])\n",
      "[[1.    0.356 1.    0.12 ]\n",
      " [1.    0.101 1.    0.12 ]\n",
      " [1.    0.046 1.    0.12 ]\n",
      " [1.    0.492 1.    0.12 ]]\n",
      "[['P' 'I-E' 'R' 'S']\n",
      " ['P' 'IE' 'R' 'S']\n",
      " ['P' 'I-E' 'R' 'S']\n",
      " ['P' 'IE' 'R' 'S']] \n",
      "\n",
      "('planes', ['p', 'l', 'ay', 'n', 'z'])\n",
      "[[1.    1.    0.129 0.967 1.   ]\n",
      " [1.    1.    0.651 0.967 0.12 ]]\n",
      "[['P' 'L' 'A' 'N' 'ES']\n",
      " ['P' 'L' 'A-E' 'N' 'S']] \n",
      "\n",
      "('poles', ['p', 'o', 'l', 'z'])\n",
      "[[1.    0.314 1.    1.   ]\n",
      " [1.    0.785 1.    0.12 ]]\n",
      "[['P' 'O' 'L' 'ES']\n",
      " ['P' 'O-E' 'L' 'S']] \n",
      "\n",
      "('populace', ['p', 'ah', 'p', ['yu', ['y', 'uh-']], 'l', 'uh-', 's'])\n",
      "[[1.    0.042 1.    0.28  1.    0.186 0.234]\n",
      " [1.    0.261 1.    0.703 1.    0.186 0.234]\n",
      " [1.    0.261 1.    0.28  1.    0.002 0.234]]\n",
      "[['P' 'O-E' 'P' 'U' 'L' 'A' 'C']\n",
      " ['P' 'O' 'P' 'U-E' 'L' 'A' 'C']\n",
      " ['P' 'O' 'P' 'U' 'L' 'A-E' 'C']] \n",
      "\n",
      "('principal', ['p', 'r', ['ih', ['ee']], 'n', 's', 'uh-', 'p', ['ul', ['uh-', 'l']]])\n",
      "[list([1.0, 1.0, 0.716, 0.9670000000000001, 0.23399999999999999, 0.18, 1.0, 1.0])\n",
      " list([1.0, 1.0, 0.005, 0.9670000000000001, 0.23399999999999999, 0.18, 1.0, 1.0])\n",
      " list([1.0, 1.0, 0.716, 0.9670000000000001, 0.23399999999999999, 0.18, 1.0, 0.18600000000000003, 1.0])\n",
      " list([1.0, 1.0, 0.005, 0.9670000000000001, 0.23399999999999999, 0.18, 1.0, 0.18600000000000003, 1.0])]\n",
      "[list(['P', 'R', 'I', 'N', 'C', 'I', 'P', 'AL'])\n",
      " list(['P', 'R', 'I', 'N', 'C', 'I', 'P', 'AL'])\n",
      " list(['P', 'R', 'I', 'N', 'C', 'I', 'P', 'A', 'L'])\n",
      " list(['P', 'R', 'I', 'N', 'C', 'I', 'P', 'A', 'L'])] \n",
      "\n",
      "('principle', ['p', 'r', ['ih', ['ee']], 'n', 's', 'uh-', 'p', ['ul', ['uh-', 'l']]])\n",
      "[[1.    1.    0.716 0.967 0.234 0.18  1.    1.   ]\n",
      " [1.    1.    0.005 0.967 0.234 0.18  1.    1.   ]]\n",
      "[['P' 'R' 'I' 'N' 'C' 'I' 'P' 'LE']\n",
      " ['P' 'R' 'I' 'N' 'C' 'I' 'P' 'LE']] \n",
      "\n",
      "('reeks', ['r', 'ee', ['ks', ['k', 's']]])\n",
      "[list([1.0, 0.252, 1.0, 0.868])\n",
      " list([1.0, 0.9790000000000001, 1.0, 0.868])]\n",
      "[list(['R', 'E-E', 'K', 'S']) list(['R', 'EE', 'K', 'S'])] \n",
      "\n",
      "('ring', ['r', ['ih', ['ee']], 'ng'])\n",
      "[[1.    0.716 1.   ]\n",
      " [1.    0.005 1.   ]]\n",
      "[['R' 'I' 'NG']\n",
      " ['R' 'I' 'NG']] \n",
      "\n",
      "('ringing', ['r', ['ih', ['ee']], 'ng', 'ih', 'ng'])\n",
      "[[1.    0.716 1.    0.716 1.   ]\n",
      " [1.    0.005 1.    0.716 1.   ]]\n",
      "[['R' 'I' 'NG' 'I' 'NG']\n",
      " ['R' 'I' 'NG' 'I' 'NG']] \n",
      "\n",
      "('roles', ['r', 'o', 'l', 'z'])\n",
      "[[1.    0.314 1.    1.   ]\n",
      " [1.    0.785 1.    0.12 ]]\n",
      "[['R' 'O' 'L' 'ES']\n",
      " ['R' 'O-E' 'L' 'S']] \n",
      "\n",
      "('sales', ['s', 'ay', 'l', 'z'])\n",
      "[[0.868 0.129 1.    1.   ]\n",
      " [0.868 0.651 1.    0.12 ]]\n",
      "[['S' 'A' 'L' 'ES']\n",
      " ['S' 'A-E' 'L' 'S']] \n",
      "\n",
      "('see', ['s', 'ee'])\n",
      "[[0.868 0.252]\n",
      " [0.868 0.979]]\n",
      "[['S' 'E-E']\n",
      " ['S' 'EE']] \n",
      "\n",
      "('seem', ['s', 'ee', 'm'])\n",
      "[[0.868 0.252 0.971]\n",
      " [0.868 0.979 0.971]]\n",
      "[['S' 'E-E' 'M']\n",
      " ['S' 'EE' 'M']] \n",
      "\n",
      "('seems', ['s', 'ee', 'm', 'z'])\n",
      "[[0.868 0.252 0.971 0.12 ]\n",
      " [0.868 0.979 0.971 0.12 ]]\n",
      "[['S' 'E-E' 'M' 'S']\n",
      " ['S' 'EE' 'M' 'S']] \n",
      "\n",
      "('seen', ['s', 'ee', 'n'])\n",
      "[[0.868 0.252 0.967]\n",
      " [0.868 0.979 0.967]]\n",
      "[['S' 'E-E' 'N']\n",
      " ['S' 'EE' 'N']] \n",
      "\n",
      "('sees', ['s', 'ee', 'z'])\n",
      "[[0.868 0.23  1.   ]\n",
      " [0.868 0.252 0.12 ]\n",
      " [0.868 0.979 0.12 ]]\n",
      "[['S' 'E' 'ES']\n",
      " ['S' 'E-E' 'S']\n",
      " ['S' 'EE' 'S']] \n",
      "\n",
      "('seller', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']]])\n",
      "[list([0.868, 0.321, 1.0, 1.0]) list([0.868, 0.419, 1.0, 0.249, 1.0])\n",
      " list([0.868, 0.419, 1.0, 0.249, 1.0])]\n",
      "[list(['S', 'E-E', 'LL', 'R']) list(['S', 'E', 'LL', 'E', 'R'])\n",
      " list(['S', 'E', 'LL', 'E', 'R'])] \n",
      "\n",
      "('sellers', ['s', 'eh', 'l', ['er', 'r', ['er', 'r']], 'z'])\n",
      "[list([0.868, 0.321, 1.0, 1.0, 0.12])\n",
      " list([0.868, 0.419, 1.0, 0.249, 1.0, 0.12])\n",
      " list([0.868, 0.419, 1.0, 0.249, 1.0, 0.12])]\n",
      "[list(['S', 'E-E', 'LL', 'R', 'S']) list(['S', 'E', 'LL', 'E', 'R', 'S'])\n",
      " list(['S', 'E', 'LL', 'E', 'R', 'S'])] \n",
      "\n",
      "('serial', ['s', ['ih', ['ee']], 'r', 'ee', ['ul', ['uh-', 'l']]])\n",
      "[list([0.868, 0.0006, 1.0, 0.005, 1.0])\n",
      " list([0.868, 0.23, 1.0, 0.005, 1.0])\n",
      " list([0.868, 0.0006, 1.0, 0.005, 0.18600000000000003, 1.0])\n",
      " list([0.868, 0.23, 1.0, 0.005, 0.18600000000000003, 1.0])]\n",
      "[list(['S', 'E', 'R', 'I', 'AL']) list(['S', 'E', 'R', 'I', 'AL'])\n",
      " list(['S', 'E', 'R', 'I', 'A', 'L']) list(['S', 'E', 'R', 'I', 'A', 'L'])] \n",
      "\n",
      "('sheer', ['sh', ['ih', ['ee']], 'r'])\n",
      "[[1.    0.002 1.   ]\n",
      " [1.    0.02  1.   ]\n",
      " [1.    0.252 1.   ]\n",
      " [1.    0.979 1.   ]]\n",
      "[['SH' 'E-E' 'R']\n",
      " ['SH' 'EE' 'R']\n",
      " ['SH' 'E-E' 'R']\n",
      " ['SH' 'EE' 'R']] \n",
      "\n",
      "('sink', ['s', ['ih', ['ee']], 'ng', 'k'])\n",
      "[[0.868 0.716 0.032 1.   ]\n",
      " [0.868 0.005 0.032 1.   ]]\n",
      "[['S' 'I' 'N' 'K']\n",
      " ['S' 'I' 'N' 'K']] \n",
      "\n",
      "('soles', ['s', 'o', 'l', 'z'])\n",
      "[[0.868 0.314 1.    1.   ]\n",
      " [0.868 0.785 1.    0.12 ]]\n",
      "[['S' 'O' 'L' 'ES']\n",
      " ['S' 'O-E' 'L' 'S']] \n",
      "\n",
      "('steel', ['s', 't', 'ee', 'l'])\n",
      "[[0.868 0.973 0.252 1.   ]\n",
      " [0.868 0.973 0.979 1.   ]]\n",
      "[['S' 'T' 'E-E' 'L']\n",
      " ['S' 'T' 'EE' 'L']] \n",
      "\n",
      "('sweets', ['s', 'w', 'ee', 't', 's'])\n",
      "[[0.868 1.    0.252 0.973 0.868]\n",
      " [0.868 1.    0.979 0.973 0.868]]\n",
      "[['S' 'W' 'E-E' 'T' 'S']\n",
      " ['S' 'W' 'EE' 'T' 'S']] \n",
      "\n",
      "('sync', ['s', ['ih', ['ee']], 'ng', 'k'])\n",
      "[[0.868 0.073 0.032 0.757]\n",
      " [0.868 0.786 0.032 0.757]]\n",
      "[['S' 'Y' 'N' 'C']\n",
      " ['S' 'Y' 'N' 'C']] \n",
      "\n",
      "('tales', ['t', 'ay', 'l', 'z'])\n",
      "[[0.973 0.129 1.    1.   ]\n",
      " [0.973 0.651 1.    0.12 ]]\n",
      "[['T' 'A' 'L' 'ES']\n",
      " ['T' 'A-E' 'L' 'S']] \n",
      "\n",
      "('tee', ['t', 'ee'])\n",
      "[[0.973 0.252]\n",
      " [0.973 0.979]]\n",
      "[['T' 'E-E']\n",
      " ['T' 'EE']] \n",
      "\n",
      "('throes', ['th-', 'r', 'o', 'z'])\n",
      "[[0.732 1.    0.314 1.   ]\n",
      " [0.732 1.    0.785 0.12 ]\n",
      " [0.732 1.    0.59  0.12 ]]\n",
      "[['TH' 'R' 'O' 'ES']\n",
      " ['TH' 'R' 'O-E' 'S']\n",
      " ['TH' 'R' 'OE' 'S']] \n",
      "\n",
      "('tic', ['t', ['ih', ['ee']], 'k'])\n",
      "[[0.973 0.716 0.757]\n",
      " [0.973 0.005 0.757]]\n",
      "[['T' 'I' 'C']\n",
      " ['T' 'I' 'C']] \n",
      "\n",
      "('tick', ['t', ['ih', ['ee']], 'k'])\n",
      "[[0.973 0.716 1.   ]\n",
      " [0.973 0.005 1.   ]]\n",
      "[['T' 'I' 'CK']\n",
      " ['T' 'I' 'CK']] \n",
      "\n",
      "('tie', ['t', 'ai'])\n",
      "[[0.973 0.589]\n",
      " [0.973 0.203]]\n",
      "[['T' 'I-E']\n",
      " ['T' 'IE']] \n",
      "\n",
      "('tied', ['t', 'ai', 'd'])\n",
      "[[0.973 0.589 0.991]\n",
      " [0.973 0.203 0.991]]\n",
      "[['T' 'I-E' 'D']\n",
      " ['T' 'IE' 'D']] \n",
      "\n",
      "('ties', ['t', 'ai', 'z'])\n",
      "[[0.973 0.074 1.   ]\n",
      " [0.973 0.589 0.12 ]\n",
      " [0.973 0.203 0.12 ]]\n",
      "[['T' 'I' 'ES']\n",
      " ['T' 'I-E' 'S']\n",
      " ['T' 'IE' 'S']] \n",
      "\n",
      "('toe', ['t', 'o'])\n",
      "[[0.973 0.785]\n",
      " [0.973 0.59 ]]\n",
      "[['T' 'O-E']\n",
      " ['T' 'OE']] \n",
      "\n",
      "('week', ['w', 'ee', 'k'])\n",
      "[[1.    0.252 1.   ]\n",
      " [1.    0.979 1.   ]]\n",
      "[['W' 'E-E' 'K']\n",
      " ['W' 'EE' 'K']] \n",
      "\n",
      "('whit', ['w', ['ih', ['ee']], 't'])\n",
      "[[0.847 0.716 0.973]\n",
      " [0.847 0.005 0.973]]\n",
      "[['WH' 'I' 'T']\n",
      " ['WH' 'I' 'T']] \n",
      "\n",
      "('wit', ['w', ['ih', ['ee']], 't'])\n",
      "[[1.    0.716 0.973]\n",
      " [1.    0.005 0.973]]\n",
      "[['W' 'I' 'T']\n",
      " ['W' 'I' 'T']] \n",
      "\n",
      "('wore', ['w', ['aw', ['o']], 'r'])\n",
      "[[1.    0.044 1.   ]\n",
      " [1.    0.785 1.   ]]\n",
      "[['W' 'O-E' 'R']\n",
      " ['W' 'O-E' 'R']] \n",
      "\n",
      "('worn', ['w', ['aw', ['o']], 'r', 'n'])\n",
      "[[1.    0.072 1.    0.967]\n",
      " [1.    0.314 1.    0.967]]\n",
      "[['W' 'O' 'R' 'N']\n",
      " ['W' 'O' 'R' 'N']] \n",
      "\n",
      "('wring', ['r', ['ih', ['ee']], 'ng'])\n",
      "[[1.    0.716 1.   ]\n",
      " [1.    0.005 1.   ]]\n",
      "[['WR' 'I' 'NG']\n",
      " ['WR' 'I' 'NG']] \n",
      "\n",
      "('wringing', ['r', ['ih', ['ee']], 'ng', 'ih', 'ng'])\n",
      "[[1.    0.716 1.    0.716 1.   ]\n",
      " [1.    0.005 1.    0.716 1.   ]]\n",
      "[['WR' 'I' 'NG' 'I' 'NG']\n",
      " ['WR' 'I' 'NG' 'I' 'NG']] \n",
      "\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i,string in enumerate(valid_grapheme_strings):\n",
    "    if len(string) >1:\n",
    "        print(hom_kcp_word_tuples[i])\n",
    "        print(valid_cond_probs[i])\n",
    "        print(string, \"\\n\")\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cond_prob_for_grapheme = word_pronunciation_predictibility.get_max_cond_prob_for_grapheme(berndt_conditional_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_score_data = word_pronunciation_predictibility.get_m_score_df(hom_kcp_word_tuples, valid_grapheme_strings,valid_cond_probs,max_cond_prob_for_grapheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'merging_dataframes' from '/Users/paule/Desktop/Gahls_Homophones_in_RedHen/merging_dataframes.py'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(merging_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging eaf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf = merging_dataframes.merge_eaf_df_to_homophone_data(homophones_in_data_celex_merged, eaf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf_video = merging_dataframes.merge_video_df_to_homophone_data(homophones_in_data_celex_eaf, video_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging gentle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf_video_gentle = merging_dataframes.merge_gentle_df_to_homophone_data(homophones_in_data_celex_eaf_video, gentle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging seg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf_video_gentle_seg = merging_dataframes.merge_seg_df_to_homophone_data(homophones_in_data_celex_eaf_video_gentle, seg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging m-scores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf_video_gentle_seg_m_scores = merging_dataframes.merge_m_scores_df_to_homophone_data(homophones_in_data_celex_eaf_video_gentle_seg,m_score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging celex syllable counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data_celex_eaf_video_gentle_seg_m_scores_syll = merging_dataframes.merge_celex_syl_counts_df_to_homophone_data(homophones_in_data_celex_eaf_video_gentle_seg_m_scores,celex_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_file', 'word', 'start', 'end', 'duration', 'label_type',\n",
       "       'mp4_error', 'aac_error', 'aac2wav_error', 'eafgz_error', 'seg_error',\n",
       "       'preceding_pause', 'subsequent_pause', 'word_frequency', 'prev_word',\n",
       "       'prev_word_frequency', 'next_word', 'next_word_frequency',\n",
       "       'length_in_letter', 'prev_word_string', 'next_word_string',\n",
       "       'prev_word_string_frequency', 'next_word_string_frequency',\n",
       "       'cond_pred_prev', 'cond_pred_next', 'has_pair', 'pron', 'celexPhon',\n",
       "       'pron_frequency', 'is_max', 'disc', 'clx', 'disc_no_bound',\n",
       "       'clx_no_bound', 'gesture', 'HandMoving', 'PersonOnScreen',\n",
       "       'SpeakerOnScreen', 'HeadMoving/MovingVertically',\n",
       "       'ShoulderMoving/NotWithHead', 'HeadMoving/MovingHorizontally',\n",
       "       'ShoulderMoving/NoSlidingWindow', 'none',\n",
       "       'ShoulderMoving/SlidingWindow', 'is_gesture', 'video_snippet_size',\n",
       "       'gentle_prev_word', 'gentle_next_word', 'gentle_end_of_sentence',\n",
       "       'gentle_start_of_sentence', 'gentle_preceding_marker',\n",
       "       'gentle_subsequent_marker', 'gentle_merging', 'gentle_index',\n",
       "       'seg_prev_word', 'seg_next_word', 'seg_end_of_sentence',\n",
       "       'seg_start_of_sentence', 'seg_preceding_marker',\n",
       "       'seg_subsequent_marker', 'seg_merging', 'seg_index', 'pos', 'rel1',\n",
       "       'rel2', 'lemma', 'm_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones_in_data_celex_eaf_video_gentle_seg_m_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_confidence_homs = homophones_in_data_celex_eaf_video_gentle_seg_m_scores[np.logical_or(homophones_in_data_celex_eaf_video_gentle_seg_m_scores.seg_merging == \"low-confidence\",homophones_in_data_celex_eaf_video_gentle_seg_m_scores.gentle_merging == \"low-confidence\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>next_word</th>\n",
       "      <th>gentle_prev_word</th>\n",
       "      <th>gentle_next_word</th>\n",
       "      <th>seg_prev_word</th>\n",
       "      <th>seg_next_word</th>\n",
       "      <th>seg_error</th>\n",
       "      <th>eafgz_error</th>\n",
       "      <th>preceding_pause</th>\n",
       "      <th>subsequent_pause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>see</td>\n",
       "      <td>let's</td>\n",
       "      <td>more</td>\n",
       "      <td>let's</td>\n",
       "      <td>if</td>\n",
       "      <td>us</td>\n",
       "      <td>if</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>time</td>\n",
       "      <td>the</td>\n",
       "      <td>been</td>\n",
       "      <td>the</td>\n",
       "      <td>they</td>\n",
       "      <td>the</td>\n",
       "      <td>they</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>time</td>\n",
       "      <td>this</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td>this</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bail</td>\n",
       "      <td>million</td>\n",
       "      <td>new</td>\n",
       "      <td>million</td>\n",
       "      <td>new</td>\n",
       "      <td>million</td>\n",
       "      <td>ellen</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>new</td>\n",
       "      <td>bail</td>\n",
       "      <td>developments</td>\n",
       "      <td>bail</td>\n",
       "      <td>developments</td>\n",
       "      <td>ellen</td>\n",
       "      <td>developments</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>new</td>\n",
       "      <td>phone</td>\n",
       "      <td>allegations</td>\n",
       "      <td>phone</td>\n",
       "      <td>allegations</td>\n",
       "      <td>ellen</td>\n",
       "      <td>allegations</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>new</td>\n",
       "      <td>is</td>\n",
       "      <td>push</td>\n",
       "      <td>a</td>\n",
       "      <td>push</td>\n",
       "      <td>a</td>\n",
       "      <td>push</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>here</td>\n",
       "      <td>were</td>\n",
       "      <td>waiting</td>\n",
       "      <td>standing</td>\n",
       "      <td>waiting</td>\n",
       "      <td>standing</td>\n",
       "      <td>waiting</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>here</td>\n",
       "      <td>down</td>\n",
       "      <td>dmv</td>\n",
       "      <td>down</td>\n",
       "      <td>dmv</td>\n",
       "      <td>down</td>\n",
       "      <td>eileen</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>here</td>\n",
       "      <td>way</td>\n",
       "      <td>is</td>\n",
       "      <td>way</td>\n",
       "      <td>is</td>\n",
       "      <td>david</td>\n",
       "      <td>is</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>rain</td>\n",
       "      <td>some</td>\n",
       "      <td>81</td>\n",
       "      <td>some</td>\n",
       "      <td>81</td>\n",
       "      <td>some</td>\n",
       "      <td>dallas</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>rain</td>\n",
       "      <td>for</td>\n",
       "      <td>i</td>\n",
       "      <td>for</td>\n",
       "      <td>i</td>\n",
       "      <td>for</td>\n",
       "      <td>dallas</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>find</td>\n",
       "      <td>to</td>\n",
       "      <td>out</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>scene</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>error</td>\n",
       "      <td>the</td>\n",
       "      <td>error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>scene</td>\n",
       "      <td>the</td>\n",
       "      <td>more</td>\n",
       "      <td>the</td>\n",
       "      <td>more</td>\n",
       "      <td>the</td>\n",
       "      <td>marc</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>way</td>\n",
       "      <td>coming</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>this</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>this</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>way</td>\n",
       "      <td>your</td>\n",
       "      <td>here</td>\n",
       "      <td>your</td>\n",
       "      <td>here</td>\n",
       "      <td>your</td>\n",
       "      <td>david</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>son</td>\n",
       "      <td>your</td>\n",
       "      <td>prop</td>\n",
       "      <td>your</td>\n",
       "      <td>prop</td>\n",
       "      <td>your</td>\n",
       "      <td>rob</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>weight</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>finding</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>--</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>--</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>sea</td>\n",
       "      <td>to</td>\n",
       "      <td>a</td>\n",
       "      <td>to</td>\n",
       "      <td>--</td>\n",
       "      <td>to</td>\n",
       "      <td>david</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>roles</td>\n",
       "      <td>supporting</td>\n",
       "      <td>film</td>\n",
       "      <td>supporting</td>\n",
       "      <td>one</td>\n",
       "      <td>supporting</td>\n",
       "      <td>one</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word   prev_word     next_word gentle_prev_word gentle_next_word  \\\n",
       "19      see       let's          more            let's               if   \n",
       "22     time         the          been              the             they   \n",
       "28     time        this           NaN             this                    \n",
       "32     bail     million           new          million              new   \n",
       "33      new        bail  developments             bail     developments   \n",
       "37      new       phone   allegations            phone      allegations   \n",
       "47      new          is          push                a             push   \n",
       "58     here        were       waiting         standing          waiting   \n",
       "59     here        down           dmv             down              dmv   \n",
       "61     here         way            is              way               is   \n",
       "93     rain        some            81             some               81   \n",
       "94     rain         for             i              for                i   \n",
       "109    find          to           out               to              the   \n",
       "113   scene         the           the              the            error   \n",
       "114   scene         the          more              the             more   \n",
       "134     way      coming     hopefully             this        hopefully   \n",
       "135     way        your          here             your             here   \n",
       "166     son        your          prop             your             prop   \n",
       "169  weight     gunfire       finding          gunfire               --   \n",
       "187     sea          to             a               to               --   \n",
       "191   roles  supporting          film       supporting              one   \n",
       "\n",
       "    seg_prev_word seg_next_word seg_error eafgz_error  preceding_pause  \\\n",
       "19             us            if  no-error    no-error            False   \n",
       "22            the          they  no-error    no-error            False   \n",
       "28           this           NaN  no-error    no-error            False   \n",
       "32        million         ellen  no-error    no-error             True   \n",
       "33          ellen  developments  no-error    no-error             True   \n",
       "37          ellen   allegations  no-error    no-error            False   \n",
       "47              a          push  no-error    no-error            False   \n",
       "58       standing       waiting  no-error    no-error            False   \n",
       "59           down        eileen  no-error    no-error            False   \n",
       "61          david            is  no-error    no-error            False   \n",
       "93           some        dallas  no-error    no-error            False   \n",
       "94            for        dallas  no-error    no-error             True   \n",
       "109            to           the  no-error    no-error            False   \n",
       "113           the         error  no-error    no-error            False   \n",
       "114           the          marc  no-error    no-error            False   \n",
       "134          this     hopefully  no-error    no-error            False   \n",
       "135          your         david  no-error    no-error            False   \n",
       "166          your           rob  no-error    no-error            False   \n",
       "169       gunfire            --  no-error    no-error            False   \n",
       "187            to         david  no-error    no-error            False   \n",
       "191    supporting           one  no-error    no-error            False   \n",
       "\n",
       "     subsequent_pause  \n",
       "19              False  \n",
       "22              False  \n",
       "28              False  \n",
       "32               True  \n",
       "33              False  \n",
       "37              False  \n",
       "47              False  \n",
       "58               True  \n",
       "59               True  \n",
       "61              False  \n",
       "93               True  \n",
       "94              False  \n",
       "109             False  \n",
       "113             False  \n",
       "114             False  \n",
       "134             False  \n",
       "135             False  \n",
       "166             False  \n",
       "169             False  \n",
       "187              True  \n",
       "191             False  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_confidence_homs[[\"word\", 'prev_word', 'next_word', 'gentle_prev_word', 'gentle_next_word', 'seg_prev_word', 'seg_next_word',\"seg_error\",'eafgz_error','preceding_pause','subsequent_pause']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
