{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Data/2016_all_words_no_audio.pickle\"\n",
    "hom_filename = \"Data/hom.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = [\"2016-12-17_1330_US_KCET_Asia_Insight\", \"2016-10-25_2300_US_KABC_Eyewitness_News_4PM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>label_type</th>\n",
       "      <th>mp4_error</th>\n",
       "      <th>aac_error</th>\n",
       "      <th>aac2wav_error</th>\n",
       "      <th>eafgz_error</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_word_frequency</th>\n",
       "      <th>next_word</th>\n",
       "      <th>next_word_frequency</th>\n",
       "      <th>letter_length</th>\n",
       "      <th>prev_word_string</th>\n",
       "      <th>next_word_string</th>\n",
       "      <th>prev_word_string_frequency</th>\n",
       "      <th>next_word_string_frequency</th>\n",
       "      <th>cond_pred_prev</th>\n",
       "      <th>cond_pred_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14828820</th>\n",
       "      <td>2016-10-25_2300_US_KABC_Eyewitness_News_4PM</td>\n",
       "      <td>police</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.38</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>don't</td>\n",
       "      <td>32647.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police-don't</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828821</th>\n",
       "      <td>2016-10-25_2300_US_KABC_Eyewitness_News_4PM</td>\n",
       "      <td>don't</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.21</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>18598.0</td>\n",
       "      <td>believe</td>\n",
       "      <td>9847.0</td>\n",
       "      <td>5</td>\n",
       "      <td>police-don't</td>\n",
       "      <td>don't-believe</td>\n",
       "      <td>37.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.054839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828822</th>\n",
       "      <td>2016-10-25_2300_US_KABC_Eyewitness_News_4PM</td>\n",
       "      <td>believe</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>32647.0</td>\n",
       "      <td>the</td>\n",
       "      <td>932396.0</td>\n",
       "      <td>7</td>\n",
       "      <td>don't-believe</td>\n",
       "      <td>believe-the</td>\n",
       "      <td>540.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>0.016541</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828823</th>\n",
       "      <td>2016-10-25_2300_US_KABC_Eyewitness_News_4PM</td>\n",
       "      <td>the</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>9847.0</td>\n",
       "      <td>mother</td>\n",
       "      <td>3407.0</td>\n",
       "      <td>3</td>\n",
       "      <td>believe-the</td>\n",
       "      <td>the-mother</td>\n",
       "      <td>793.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>0.080532</td>\n",
       "      <td>0.143822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14828824</th>\n",
       "      <td>2016-10-25_2300_US_KABC_Eyewitness_News_4PM</td>\n",
       "      <td>mother</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.39</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>932396.0</td>\n",
       "      <td>or</td>\n",
       "      <td>57737.0</td>\n",
       "      <td>6</td>\n",
       "      <td>the-mother</td>\n",
       "      <td>mother-or</td>\n",
       "      <td>490.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17805800</th>\n",
       "      <td>2016-12-17_1330_US_KCET_Asia_Insight</td>\n",
       "      <td>2017</td>\n",
       "      <td>1657.55</td>\n",
       "      <td>1657.71</td>\n",
       "      <td>0.16</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>335519.0</td>\n",
       "      <td>as</td>\n",
       "      <td>89095.0</td>\n",
       "      <td>4</td>\n",
       "      <td>in-2017</td>\n",
       "      <td>2017-as</td>\n",
       "      <td>151.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17805801</th>\n",
       "      <td>2016-12-17_1330_US_KCET_Asia_Insight</td>\n",
       "      <td>as</td>\n",
       "      <td>1659.25</td>\n",
       "      <td>1659.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>597.0</td>\n",
       "      <td>the</td>\n",
       "      <td>932396.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-as</td>\n",
       "      <td>as-the</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6909.0</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.007410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17805802</th>\n",
       "      <td>2016-12-17_1330_US_KCET_Asia_Insight</td>\n",
       "      <td>the</td>\n",
       "      <td>1659.50</td>\n",
       "      <td>1659.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>89095.0</td>\n",
       "      <td>world's</td>\n",
       "      <td>1593.0</td>\n",
       "      <td>3</td>\n",
       "      <td>as-the</td>\n",
       "      <td>the-world's</td>\n",
       "      <td>6909.0</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.077546</td>\n",
       "      <td>0.794099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17805803</th>\n",
       "      <td>2016-12-17_1330_US_KCET_Asia_Insight</td>\n",
       "      <td>world's</td>\n",
       "      <td>1659.65</td>\n",
       "      <td>1660.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>932396.0</td>\n",
       "      <td>largest</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>7</td>\n",
       "      <td>the-world's</td>\n",
       "      <td>world's-largest</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.110604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17805804</th>\n",
       "      <td>2016-12-17_1330_US_KCET_Asia_Insight</td>\n",
       "      <td>largest</td>\n",
       "      <td>1660.25</td>\n",
       "      <td>1660.82</td>\n",
       "      <td>0.57</td>\n",
       "      <td>high-confidence</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>no-error</td>\n",
       "      <td>...</td>\n",
       "      <td>1593.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>world's-largest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136849</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7435 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file     word    start  \\\n",
       "14828820  2016-10-25_2300_US_KABC_Eyewitness_News_4PM   police     0.29   \n",
       "14828821  2016-10-25_2300_US_KABC_Eyewitness_News_4PM    don't     0.67   \n",
       "14828822  2016-10-25_2300_US_KABC_Eyewitness_News_4PM  believe     0.88   \n",
       "14828823  2016-10-25_2300_US_KABC_Eyewitness_News_4PM      the     1.22   \n",
       "14828824  2016-10-25_2300_US_KABC_Eyewitness_News_4PM   mother     1.40   \n",
       "...                                               ...      ...      ...   \n",
       "17805800         2016-12-17_1330_US_KCET_Asia_Insight     2017  1657.55   \n",
       "17805801         2016-12-17_1330_US_KCET_Asia_Insight       as  1659.25   \n",
       "17805802         2016-12-17_1330_US_KCET_Asia_Insight      the  1659.50   \n",
       "17805803         2016-12-17_1330_US_KCET_Asia_Insight  world's  1659.65   \n",
       "17805804         2016-12-17_1330_US_KCET_Asia_Insight  largest  1660.25   \n",
       "\n",
       "              end  duration       label_type mp4_error aac_error  \\\n",
       "14828820     0.67      0.38  high-confidence  no-error  no-error   \n",
       "14828821     0.88      0.21  high-confidence  no-error  no-error   \n",
       "14828822     1.21      0.33  high-confidence  no-error  no-error   \n",
       "14828823     1.40      0.18  high-confidence  no-error  no-error   \n",
       "14828824     1.79      0.39  high-confidence  no-error  no-error   \n",
       "...           ...       ...              ...       ...       ...   \n",
       "17805800  1657.71      0.16  high-confidence  no-error  no-error   \n",
       "17805801  1659.49      0.24  high-confidence  no-error  no-error   \n",
       "17805802  1659.65      0.15  high-confidence  no-error  no-error   \n",
       "17805803  1660.25      0.60  high-confidence  no-error  no-error   \n",
       "17805804  1660.82      0.57  high-confidence  no-error  no-error   \n",
       "\n",
       "         aac2wav_error eafgz_error  ... prev_word_frequency  next_word  \\\n",
       "14828820      no-error    no-error  ...                 NaN      don't   \n",
       "14828821      no-error    no-error  ...             18598.0    believe   \n",
       "14828822      no-error    no-error  ...             32647.0        the   \n",
       "14828823      no-error    no-error  ...              9847.0     mother   \n",
       "14828824      no-error    no-error  ...            932396.0         or   \n",
       "...                ...         ...  ...                 ...        ...   \n",
       "17805800      no-error    no-error  ...            335519.0         as   \n",
       "17805801      no-error    no-error  ...               597.0        the   \n",
       "17805802      no-error    no-error  ...             89095.0    world's   \n",
       "17805803      no-error    no-error  ...            932396.0    largest   \n",
       "17805804      no-error    no-error  ...              1593.0        NaN   \n",
       "\n",
       "          next_word_frequency  letter_length prev_word_string  \\\n",
       "14828820              32647.0              6              NaN   \n",
       "14828821               9847.0              5     police-don't   \n",
       "14828822             932396.0              7    don't-believe   \n",
       "14828823               3407.0              3      believe-the   \n",
       "14828824              57737.0              6       the-mother   \n",
       "...                       ...            ...              ...   \n",
       "17805800              89095.0              4          in-2017   \n",
       "17805801             932396.0              2          2017-as   \n",
       "17805802               1593.0              3           as-the   \n",
       "17805803               1971.0              7      the-world's   \n",
       "17805804                  NaN              7  world's-largest   \n",
       "\n",
       "          next_word_string prev_word_string_frequency  \\\n",
       "14828820      police-don't                        NaN   \n",
       "14828821     don't-believe                       37.0   \n",
       "14828822       believe-the                      540.0   \n",
       "14828823        the-mother                      793.0   \n",
       "14828824         mother-or                      490.0   \n",
       "...                    ...                        ...   \n",
       "17805800           2017-as                      151.0   \n",
       "17805801            as-the                       15.0   \n",
       "17805802       the-world's                     6909.0   \n",
       "17805803   world's-largest                     1265.0   \n",
       "17805804               NaN                      218.0   \n",
       "\n",
       "          next_word_string_frequency  cond_pred_prev cond_pred_next  \n",
       "14828820                        37.0             NaN       0.001133  \n",
       "14828821                       540.0        0.001989       0.054839  \n",
       "14828822                       793.0        0.016541       0.000850  \n",
       "14828823                       490.0        0.080532       0.143822  \n",
       "14828824                        21.0        0.000526       0.000364  \n",
       "...                              ...             ...            ...  \n",
       "17805800                        15.0        0.000450       0.000168  \n",
       "17805801                      6909.0        0.025126       0.007410  \n",
       "17805802                      1265.0        0.077546       0.794099  \n",
       "17805803                       218.0        0.001357       0.110604  \n",
       "17805804                         NaN        0.136849            NaN  \n",
       "\n",
       "[7435 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.read_csv(\"sub_df.csv\", index_col=\"Unnamed: 0\")\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and extract information from eaf files...\n",
      "Files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Files:  50%|█████     | 1/2 [00:01<00:01,  1.49s/it]ews_4PM\u001b[A\n",
      "Processed file: 2016-12-17_1330_US_KCET_Asia_Insight       \u001b[A\n",
      "Files: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "Load and extract information from seg files...\n",
      "Files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Processed file: 2016-10-25_2300_US_KABC_Eyewitness_News_4PM\u001b[A\n",
      "Processed file: 2016-12-17_1330_US_KCET_Asia_Insight       \u001b[A\n",
      "Files: 100%|██████████| 2/2 [00:00<00:00, 13.52it/s]\n",
      "Load and extract information from gentle files...\n",
      "Files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Files:  50%|█████     | 1/2 [00:00<00:00,  3.46it/s]ews_4PM\u001b[A\n",
      "Processed file: 2016-12-17_1330_US_KCET_Asia_Insight       \u001b[A\n",
      "Files: 100%|██████████| 2/2 [00:00<00:00,  4.92it/s]\n"
     ]
    }
   ],
   "source": [
    "eaf_data = preprocessing.get_additional_data_from_files(sub_df, \"eaf\")\n",
    "seg_data = preprocessing.get_additional_data_from_files(sub_df, \"seg\")\n",
    "gentle_data = preprocessing.get_additional_data_from_files(sub_df, \"gentle\")\n",
    "video_data =  pd.read_csv(\"sub_video_data.csv\", index_col=\"Unnamed: 0\") #preprocessing.get_additional_data_from_files(sub_df, \"video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read Gahls Homophone data from Data/hom.csv\n",
      "65 out of 412 homophones found in Data:\n",
      "Homophone Pairs found in Data: 6\n",
      "Homophones without Pair:  ['ads', 'bail', 'bear', 'blue', 'board', 'cell', 'chilly', 'course', 'crews', 'die', 'died', 'doe', 'due', 'fair', 'feet', 'fill', 'find', 'great', 'hall', 'higher', 'hold', 'jim', 'knows', 'made', 'meet', 'meets', 'morning', 'night', 'piece', 'poll', 'rain', 'right', 'rights', 'ring', 'road', 'roles', 'sales', 'scene', 'seem', 'seems', 'sees', 'sent', 'shoot', 'son', 'straight', 'tax', 'time', 'waiting', 'waste', 'way', 'ways', 'week', 'whole']\n",
      "Missing homophones: ['ad' 'add' 'adds' 'aid' 'aide' 'aides' 'aids' 'airs' 'allowed' 'aloud'\n",
      " 'baits' 'bald' 'bale' 'band' 'banned' 'bare' 'bates' 'bawled' 'beats'\n",
      " 'beets' 'bell' 'belle' 'berry' 'billed' 'blew' 'boar' 'bold' 'bore'\n",
      " 'bored' 'bowled' 'brakes' 'bread' 'breaks' 'bred' 'build' 'bury'\n",
      " 'callous' 'callus' 'capital' 'capitol' 'ceiling' 'cellar' 'cellars'\n",
      " 'cells' 'cent' 'cents' 'cereal' 'chews' 'chile' 'choose' 'chord' 'chords'\n",
      " 'chute' 'chutes' 'coarse' 'coco' 'cocoa' 'cord' 'cords' 'core' 'corps'\n",
      " 'council' 'counsel' 'cruise' 'dear' 'deer' 'dew' 'dies' 'doc' 'dock'\n",
      " 'dough' 'drier' 'dryer' 'dye' 'dyed' 'dyes' 'elicit' 'ensure' 'fare'\n",
      " 'feat' 'fiance' 'fiancee' 'fined' 'flair' 'flare' 'flea' 'flee' 'flew'\n",
      " 'flour' 'flours' 'flower' 'flowers' 'flu' 'franc' 'frank' 'grate'\n",
      " 'grisly' 'grizzly' 'guessed' 'guest' 'guise' 'guys' 'gym' 'halls' 'hart'\n",
      " 'haul' 'hauls' 'heal' 'heals' 'heard' 'heart' 'heel' 'heels' 'heirs'\n",
      " 'herd' 'hertz' 'hire' 'hoarse' 'hole' 'holed' 'holes' 'horse' 'hurts'\n",
      " 'illicit' 'insure' 'kneading' 'knight' 'knights' 'knit' 'lacks' 'lax'\n",
      " 'lea' 'leased' 'least' 'lee' 'lessen' 'lesson' 'levee' 'levy' 'loan'\n",
      " 'lone' 'lumbar' 'lumber' 'mac' 'mach' 'maid' 'mail' 'mails' 'male'\n",
      " 'males' 'mall' 'manner' 'manor' 'marks' 'marx' 'maul' 'meat' 'meats'\n",
      " 'missed' 'mist' 'mode' 'moose' 'mourning' 'mousse' 'mowed' 'naval'\n",
      " 'navel' 'needing' 'nights' 'nit' 'nose' 'paced' 'packed' 'pact' 'pain'\n",
      " 'pains' 'pair' 'pairs' 'pane' 'panes' 'paste' 'pause' 'paws' 'pea'\n",
      " 'peace' 'peak' 'peaked' 'pear' 'pears' 'pee' 'peek' 'peer' 'peers' 'phil'\n",
      " 'pi' 'pie' 'pier' 'piers' 'piqued' 'plain' 'plains' 'plane' 'planes'\n",
      " 'plum' 'plumb' 'pole' 'poles' 'polls' 'populace' 'populous' 'praise'\n",
      " 'pray' 'praying' 'prey' 'preying' 'preys' 'principal' 'principle' 'puts'\n",
      " 'putts' 'rack' 'rained' 'rains' 'rap' 'reeks' 'reigned' 'rein' 'reins'\n",
      " 'ringing' 'roam' 'rode' 'rolls' 'rome' 'rose' 'rote' 'rough' 'rows'\n",
      " 'ruff' 'sac' 'sack' 'sacks' 'sail' 'sails' 'sale' 'sax' 'scents'\n",
      " 'sealing' 'seam' 'seams' 'seas' 'seen' 'sell' 'seller' 'sellers' 'sells'\n",
      " 'serial' 'shear' 'sheer' 'shoots' 'sight' 'sighted' 'sights' 'sink'\n",
      " 'site' 'sited' 'sites' 'slay' 'sleigh' 'sole' 'soles' 'sons' 'soul'\n",
      " 'souls' 'spade' 'spayed' 'stake' 'stakes' 'steak' 'steaks' 'steal'\n",
      " 'steel' 'strait' 'suede' 'suites' 'sun' 'suns' 'swayed' 'sweets' 'sync'\n",
      " 'tacks' 'tail' 'tails' 'tale' 'tales' 'taught' 'taut' 'tea' 'teas'\n",
      " 'tease' 'tec' 'tech' 'tee' 'thai' 'thais' 'throes' 'throws' 'thyme' 'tic'\n",
      " 'tick' 'tide' 'tie' 'tied' 'ties' 'toe' 'tow' 'tracked' 'tract' 'wade'\n",
      " 'waist' 'waits' 'waive' 'war' 'warn' 'wave' 'wax' 'weak' 'weigh'\n",
      " 'weighed' 'weighs' 'weighting' 'weights' 'whacks' 'whine' 'whit' 'wholes'\n",
      " 'wine' 'wit' 'wore' 'worn' 'wrack' 'wrap' 'wreaks' 'wring' 'wringing'\n",
      " 'write' 'writes' 'wrote']\n"
     ]
    }
   ],
   "source": [
    "homophones_in_data, gahls_homophones, gahls_homophones_missing_in_data = preprocessing.read_and_extract_homophones(hom_filename, sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data.sort_values(by = [\"source_file\", \"start\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_file', 'word', 'start', 'end', 'duration', 'label_type',\n",
       "       'mp4_error', 'aac_error', 'aac2wav_error', 'eafgz_error', 'seg_error',\n",
       "       'preceding_pause', 'subsequent_pause', 'word_frequency', 'prev_word',\n",
       "       'prev_word_frequency', 'next_word', 'next_word_frequency',\n",
       "       'letter_length', 'prev_word_string', 'next_word_string',\n",
       "       'prev_word_string_frequency', 'next_word_string_frequency',\n",
       "       'cond_pred_prev', 'cond_pred_next', 'has_pair', 'pron', 'celexPhon',\n",
       "       'pron_frequency', 'is_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homophones_in_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge EAF and Video Dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and sort homophones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eaf_hom_data = eaf_data[eaf_data['annotation'].isin(np.unique(homophones_in_data[\"word\"]))].sort_values(by = [\"source_file\", \"start\"])\n",
    "video_hom_data = video_data[video_data['word'].isin(np.unique(homophones_in_data[\"word\"]))].sort_values(by = [\"source_file\", \"start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eaf_hom_data.sort_values(by = [\"source_file\", \"start\"], inplace = True)\n",
    "video_hom_data.sort_values(by = [\"source_file\", \"start\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### make sure that start times match in units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones_in_data.start = homophones_in_data.start.round(2)\n",
    "eaf_hom_data.start = (eaf_hom_data.start/1000).round(2)\n",
    "eaf_hom_data.end = (eaf_hom_data.end/1000).round(2)\n",
    "video_hom_data.start = video_hom_data.start.round(2)\n",
    "eaf_hom_data.rename(columns={\"annotation\": \"word\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge eaf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_data_eaf = homophones_in_data.merge(eaf_hom_data[['word', 'source_file', 'start','gesture','HandMoving', 'PersonOnScreen',\n",
    "       'SpeakerOnScreen', 'HeadMoving/MovingVertically',\n",
    "       'ShoulderMoving/NotWithHead', 'HeadMoving/MovingHorizontally',\n",
    "       'ShoulderMoving/NoSlidingWindow', 'none',\n",
    "       'ShoulderMoving/SlidingWindow', 'is_gesture']], on =[\"source_file\",\"start\", \"word\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_data_eaf_video = hom_data_eaf.merge(video_hom_data[['source_file', 'word', 'start', 'video_snippet_size']], on =[\"source_file\",\"start\", \"word\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Seg and Gentle Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentle_data[\"IDX\"] = gentle_data.index # no start time information included\n",
    "gentle_data[\"word\"] = gentle_data.word.str.lower()\n",
    "gentle_data[\"prev_word\"] = gentle_data.prev_word.str.lower()\n",
    "gentle_data[\"next_word\"] =  gentle_data.next_word.str.lower()\n",
    "\n",
    "seg_data[\"IDX\"] = seg_data.index\n",
    "seg_data[\"word\"] = seg_data.word.str.lower()\n",
    "seg_data[\"prev_word\"] = seg_data.prev_word.str.lower()\n",
    "seg_data[\"next_word\"] = seg_data.next_word.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and sort homophones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_hom_data = seg_data[seg_data['word'].isin(np.unique(homophones_in_data[\"word\"]))].sort_values(by = [\"source_file\", \"IDX\"])\n",
    "gentle_hom_data = gentle_data[gentle_data['word'].isin(np.unique(homophones_in_data[\"word\"]))].sort_values(by = [\"source_file\", \"IDX\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge gentle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_gentle_to_homophone_data(homophones_in_data, gentle_hom_data):\n",
    "    hom_data_gentle = None\n",
    "    for file in np.unique(homophones_in_data.source_file):\n",
    "        homophones_in_data_i = homophones_in_data[homophones_in_data.source_file == file].sort_values(by = [\"start\"])\n",
    "        gentle_hom_data_i = gentle_hom_data[gentle_hom_data.source_file == file].sort_values(by = [\"IDX\"])\n",
    "\n",
    "        homophones_in_data_i[\"gentle_prev_word\"] = None\n",
    "        homophones_in_data_i[\"gentle_next_word\"] = None\n",
    "        homophones_in_data_i[\"gentle_end_of_sentence\"] = None\n",
    "        homophones_in_data_i[\"gentle_start_of_sentence\"] = None\n",
    "        homophones_in_data_i[\"gentle_preceding_marker\"] = None\n",
    "        homophones_in_data_i[\"gentle_subsequent_marker\"] = None\n",
    "        homophones_in_data_i[\"gentle_merging\"] = None\n",
    "        homophones_in_data_i[\"gentle_index\"] = None\n",
    "\n",
    "        for row_index, row in homophones_in_data_i.iterrows():\n",
    "            hom = row[\"word\"]\n",
    "            prev_word = row[\"prev_word\"]\n",
    "            next_word = row[\"next_word\"]\n",
    "\n",
    "            possible_matches = gentle_hom_data_i[gentle_hom_data_i.word == hom].index\n",
    "\n",
    "            for idx in possible_matches:\n",
    "                possible_row = gentle_hom_data_i.loc[idx]\n",
    "\n",
    "                if possible_row.prev_word == prev_word and possible_row.next_word == next_word:\n",
    "                    homophones_in_data_i.at[row_index, 'gentle_merging'] = \"high-confidence\"\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_end_of_sentence\"] = possible_row.end_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_start_of_sentence\"] = possible_row.start_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_preceding_marker\"] = possible_row.preceding_marker\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_subsequent_marker\"] = possible_row.subsequent_marker\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_index\"] = possible_row.IDX\n",
    "                    gentle_hom_data_i.drop(index=idx,inplace=True)\n",
    "                    break\n",
    "\n",
    "                elif possible_row.prev_word == prev_word or possible_row.next_word == next_word:\n",
    "                    #print(possible_row.prev_word, possible_row.word, possible_row.next_word)\n",
    "                    homophones_in_data_i.at[row_index,\"gentle_prev_word\"] = possible_row.prev_word\n",
    "                    homophones_in_data_i.at[row_index,\"gentle_next_word\"] = possible_row.next_word\n",
    "                    homophones_in_data_i.at[row_index, 'gentle_merging'] = \"low-confidence\"\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_end_of_sentence\"] = possible_row.end_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_start_of_sentence\"] = possible_row.start_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_preceding_marker\"] = possible_row.preceding_marker\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_subsequent_marker\"] = possible_row.subsequent_marker\n",
    "                    homophones_in_data_i.at[row_index, \"gentle_index\"] = possible_row.IDX\n",
    "                    gentle_hom_data_i.drop(index=idx,inplace=True)\n",
    "                    break\n",
    "\n",
    "        if hom_data_gentle is None:\n",
    "            hom_data_gentle = homophones_in_data_i\n",
    "        else:\n",
    "            hom_data_gentle = pd.concat([hom_data_gentle, homophones_in_data_i], )\n",
    "\n",
    "\n",
    "    hom_data_gentle.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "    return hom_data_gentle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_data_eaf_video_gentle = merge_gentle_to_homophone_data(hom_data_eaf_video, gentle_hom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>gentle_prev_word</th>\n",
       "      <th>next_word</th>\n",
       "      <th>gentle_next_word</th>\n",
       "      <th>gentle_merging</th>\n",
       "      <th>gentle_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>find</td>\n",
       "      <td>veterans</td>\n",
       "      <td>veterans</td>\n",
       "      <td>good</td>\n",
       "      <td>good-paying</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>1490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>time</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>been</td>\n",
       "      <td>they</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>2171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>here</td>\n",
       "      <td>were</td>\n",
       "      <td>standing</td>\n",
       "      <td>waiting</td>\n",
       "      <td>waiting</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>3371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>find</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>out</td>\n",
       "      <td>the</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>3578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>weight</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>finding</td>\n",
       "      <td>--</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>way</td>\n",
       "      <td>coming</td>\n",
       "      <td>this</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>sent</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>abc</td>\n",
       "      <td>abc-7</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>5337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sea</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>a</td>\n",
       "      <td>--</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>new</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>push</td>\n",
       "      <td>push</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>scene</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>error</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>see</td>\n",
       "      <td>let's</td>\n",
       "      <td>let's</td>\n",
       "      <td>more</td>\n",
       "      <td>if</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>time</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>roles</td>\n",
       "      <td>supporting</td>\n",
       "      <td>supporting</td>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7275.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word   prev_word gentle_prev_word  next_word gentle_next_word  \\\n",
       "23     find    veterans         veterans       good      good-paying   \n",
       "38     time         the              the       been             they   \n",
       "79     here        were         standing    waiting          waiting   \n",
       "88     find          to               to        out              the   \n",
       "100  weight     gunfire          gunfire    finding               --   \n",
       "109     way      coming             this  hopefully        hopefully   \n",
       "132    sent   statement        statement        abc            abc-7   \n",
       "156     sea          to               to          a               --   \n",
       "163     new          is                a       push             push   \n",
       "166   scene         the              the        the            error   \n",
       "168     see       let's            let's       more               if   \n",
       "170    time        this             this        NaN                    \n",
       "181   roles  supporting       supporting       film              one   \n",
       "\n",
       "     gentle_merging  gentle_index  \n",
       "23   low-confidence        1490.0  \n",
       "38   low-confidence        2171.0  \n",
       "79   low-confidence        3371.0  \n",
       "88   low-confidence        3578.0  \n",
       "100  low-confidence        4236.0  \n",
       "109  low-confidence        4533.0  \n",
       "132  low-confidence        5337.0  \n",
       "156  low-confidence        6307.0  \n",
       "163  low-confidence        6744.0  \n",
       "166  low-confidence        6859.0  \n",
       "168  low-confidence        6891.0  \n",
       "170  low-confidence        6975.0  \n",
       "181  low-confidence        7275.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom_data_eaf_video_gentle[hom_data_eaf_video_gentle.gentle_merging == \"low-confidence\"][[\"word\", \"prev_word\", \"gentle_prev_word\", \"next_word\", \"gentle_next_word\", \"gentle_merging\", 'gentle_index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge seg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seg_to_homophone_data(homophones_in_data, seg_hom_data):\n",
    "    hom_data_seg = None        \n",
    "    for file in np.unique(homophones_in_data.source_file):\n",
    "        homophones_in_data_i = homophones_in_data[homophones_in_data.source_file == file].sort_values(by = [\"start\"])\n",
    "        seg_hom_data_i = seg_hom_data[seg_hom_data.source_file == file].sort_values(by = [\"IDX\"])\n",
    "\n",
    "        homophones_in_data_i[\"seg_prev_word\"] = None\n",
    "        homophones_in_data_i[\"seg_next_word\"] = None\n",
    "        homophones_in_data_i[\"seg_end_of_sentence\"] = None\n",
    "        homophones_in_data_i[\"seg_start_of_sentence\"] = None\n",
    "        homophones_in_data_i[\"seg_preceding_marker\"] = None\n",
    "        homophones_in_data_i[\"seg_subsequent_marker\"] = None\n",
    "        homophones_in_data_i[\"seg_merging\"] = None\n",
    "        homophones_in_data_i[\"seg_index\"] = None\n",
    "        homophones_in_data_i[\"pos\"] = None\n",
    "        homophones_in_data_i[\"rel1\"] = None\n",
    "        homophones_in_data_i[\"rel2\"] = None\n",
    "        homophones_in_data_i[\"lemma\"] = None\n",
    "\n",
    "\n",
    "        for row_index, row in homophones_in_data_i.iterrows():\n",
    "            hom = row[\"word\"]\n",
    "            prev_word = row[\"prev_word\"]\n",
    "            next_word = row[\"next_word\"]\n",
    "\n",
    "            possible_matches = seg_hom_data_i[seg_hom_data_i.word == hom].index\n",
    "\n",
    "            for idx in possible_matches:\n",
    "                possible_row = seg_hom_data_i.loc[idx]\n",
    "\n",
    "                if possible_row.prev_word == prev_word and possible_row.next_word == next_word:\n",
    "                    homophones_in_data_i.at[row_index, 'seg_merging'] = \"high-confidence\"\n",
    "                    homophones_in_data_i.at[row_index, \"seg_end_of_sentence\"] = possible_row.end_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"seg_start_of_sentence\"] = possible_row.start_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"seg_preceding_marker\"] = possible_row.preceding_marker\n",
    "                    homophones_in_data_i.at[row_index, \"seg_subsequent_marker\"] = possible_row.subsequent_marker\n",
    "                    homophones_in_data_i.at[row_index, \"seg_index\"] = possible_row.IDX\n",
    "                    homophones_in_data_i.at[row_index, \"pos\"] = possible_row.pos\n",
    "                    homophones_in_data_i.at[row_index, \"rel1\"] = possible_row.rel1\n",
    "                    homophones_in_data_i.at[row_index, \"rel2\"] = possible_row.rel2\n",
    "                    homophones_in_data_i.at[row_index, \"lemma\"] = possible_row.lemma\n",
    "\n",
    "                    seg_hom_data_i.drop(index=idx,inplace=True)\n",
    "                    break\n",
    "\n",
    "                elif possible_row.prev_word == prev_word or possible_row.next_word == next_word:\n",
    "\n",
    "                    homophones_in_data_i.at[row_index, \"seg_prev_word\"] = possible_row.prev_word\n",
    "                    homophones_in_data_i.at[row_index, \"seg_next_word\"] = possible_row.next_word\n",
    "                    homophones_in_data_i.at[row_index, 'seg_merging'] = \"low-confidence\"\n",
    "                    homophones_in_data_i.at[row_index, \"seg_end_of_sentence\"] = possible_row.end_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"seg_start_of_sentence\"] = possible_row.start_of_sentence\n",
    "                    homophones_in_data_i.at[row_index, \"seg_preceding_marker\"] = possible_row.preceding_marker\n",
    "                    homophones_in_data_i.at[row_index, \"seg_subsequent_marker\"] = possible_row.subsequent_marker\n",
    "                    homophones_in_data_i.at[row_index, \"seg_index\"] = possible_row.IDX\n",
    "                    homophones_in_data_i.at[row_index, \"pos\"] = possible_row.pos\n",
    "                    homophones_in_data_i.at[row_index, \"rel1\"] = possible_row.rel1\n",
    "                    homophones_in_data_i.at[row_index, \"rel2\"] = possible_row.rel2\n",
    "                    homophones_in_data_i.at[row_index, \"lemma\"] = possible_row.lemma\n",
    "                    seg_hom_data_i.drop(index=idx,inplace=True)\n",
    "                    break\n",
    "\n",
    "        if hom_data_seg is None:\n",
    "            hom_data_seg = homophones_in_data_i\n",
    "        else:\n",
    "            hom_data_seg = pd.concat([hom_data_seg, homophones_in_data_i], ) \n",
    "\n",
    "    hom_data_seg.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "    return hom_data_seg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hom_data_eaf_video_gentle_seg = merge_seg_to_homophone_data(hom_data_eaf_video_gentle, seg_hom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>seg_prev_word</th>\n",
       "      <th>next_word</th>\n",
       "      <th>seg_next_word</th>\n",
       "      <th>seg_merging</th>\n",
       "      <th>seg_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>see</td>\n",
       "      <td>didn't</td>\n",
       "      <td>not</td>\n",
       "      <td>people</td>\n",
       "      <td>people</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bail</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>new</td>\n",
       "      <td>ellen</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>new</td>\n",
       "      <td>bail</td>\n",
       "      <td>ellen</td>\n",
       "      <td>developments</td>\n",
       "      <td>developments</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rain</td>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "      <td>81</td>\n",
       "      <td>dallas</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>1406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>find</td>\n",
       "      <td>veterans</td>\n",
       "      <td>veterans</td>\n",
       "      <td>good</td>\n",
       "      <td>good-paying</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>1550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>wait</td>\n",
       "      <td>don't</td>\n",
       "      <td>not</td>\n",
       "      <td>until</td>\n",
       "      <td>until</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>2172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>time</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>been</td>\n",
       "      <td>they</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>2267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>rain</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>i</td>\n",
       "      <td>dallas</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>2464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>here</td>\n",
       "      <td>were</td>\n",
       "      <td>standing</td>\n",
       "      <td>waiting</td>\n",
       "      <td>waiting</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>3517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>here</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>dmv</td>\n",
       "      <td>eileen</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>3757.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>find</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>out</td>\n",
       "      <td>the</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>3731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>son</td>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>prop</td>\n",
       "      <td>rob</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>see</td>\n",
       "      <td>hurt</td>\n",
       "      <td>hurt</td>\n",
       "      <td>i'm</td>\n",
       "      <td>i</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>weight</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>gunfire</td>\n",
       "      <td>finding</td>\n",
       "      <td>--</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>way</td>\n",
       "      <td>coming</td>\n",
       "      <td>this</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>hopefully</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>way</td>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>here</td>\n",
       "      <td>david</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>here</td>\n",
       "      <td>way</td>\n",
       "      <td>david</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>new</td>\n",
       "      <td>phone</td>\n",
       "      <td>ellen</td>\n",
       "      <td>allegations</td>\n",
       "      <td>allegations</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>4899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>time</td>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td>doesn't</td>\n",
       "      <td>does</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>5069.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>sent</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>abc</td>\n",
       "      <td>abc-7</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>5571.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>see</td>\n",
       "      <td>you'll</td>\n",
       "      <td>will</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>5967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>course</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>it's</td>\n",
       "      <td>it</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6058.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>raise</td>\n",
       "      <td>doesn't</td>\n",
       "      <td>not</td>\n",
       "      <td>taxes</td>\n",
       "      <td>taxes</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sea</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>a</td>\n",
       "      <td>david</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>wait</td>\n",
       "      <td>don't</td>\n",
       "      <td>not</td>\n",
       "      <td>until</td>\n",
       "      <td>until</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>6860.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>new</td>\n",
       "      <td>is</td>\n",
       "      <td>a</td>\n",
       "      <td>push</td>\n",
       "      <td>push</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>scene</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>error</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>scene</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>more</td>\n",
       "      <td>marc</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>scene</td>\n",
       "      <td>crime</td>\n",
       "      <td>crime</td>\n",
       "      <td>completely's</td>\n",
       "      <td>completely</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>time</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>roles</td>\n",
       "      <td>supporting</td>\n",
       "      <td>supporting</td>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>seem</td>\n",
       "      <td>doesn't</td>\n",
       "      <td>not</td>\n",
       "      <td>interested</td>\n",
       "      <td>interested</td>\n",
       "      <td>low-confidence</td>\n",
       "      <td>7763.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word   prev_word seg_prev_word     next_word seg_next_word  \\\n",
       "1       see      didn't           not        people        people   \n",
       "7      bail     million       million           new         ellen   \n",
       "8       new        bail         ellen  developments  developments   \n",
       "21     rain        some          some            81        dallas   \n",
       "23     find    veterans      veterans          good   good-paying   \n",
       "37     wait       don't           not         until         until   \n",
       "38     time         the           the          been          they   \n",
       "41     rain         for           for             i        dallas   \n",
       "79     here        were      standing       waiting       waiting   \n",
       "87     here        down          down           dmv        eileen   \n",
       "88     find          to            to           out           the   \n",
       "92      son        your          your          prop           rob   \n",
       "97      see        hurt          hurt           i'm             i   \n",
       "100  weight     gunfire       gunfire       finding            --   \n",
       "109     way      coming          this     hopefully     hopefully   \n",
       "112     way        your          your          here         david   \n",
       "113    here         way         david            is            is   \n",
       "117     new       phone         ellen   allegations   allegations   \n",
       "120    time        more          more       doesn't          does   \n",
       "132    sent   statement     statement           abc         abc-7   \n",
       "141     see      you'll          will            it            it   \n",
       "144  course          of            of          it's            it   \n",
       "152   raise     doesn't           not         taxes         taxes   \n",
       "156     sea          to            to             a         david   \n",
       "161    wait       don't           not         until         until   \n",
       "163     new          is             a          push          push   \n",
       "166   scene         the           the           the         error   \n",
       "167   scene         the           the          more          marc   \n",
       "169   scene       crime         crime  completely's    completely   \n",
       "170    time        this          this           NaN           NaN   \n",
       "181   roles  supporting    supporting          film           one   \n",
       "184    seem     doesn't           not    interested    interested   \n",
       "\n",
       "        seg_merging  seg_index  \n",
       "1    low-confidence       86.0  \n",
       "7    low-confidence      316.0  \n",
       "8    low-confidence      320.0  \n",
       "21   low-confidence     1406.0  \n",
       "23   low-confidence     1550.0  \n",
       "37   low-confidence     2172.0  \n",
       "38   low-confidence     2267.0  \n",
       "41   low-confidence     2464.0  \n",
       "79   low-confidence     3517.0  \n",
       "87   low-confidence     3757.0  \n",
       "88   low-confidence     3731.0  \n",
       "92   low-confidence     4025.0  \n",
       "97   low-confidence     4349.0  \n",
       "100  low-confidence     4417.0  \n",
       "109  low-confidence     4726.0  \n",
       "112  low-confidence     4806.0  \n",
       "113  low-confidence     4810.0  \n",
       "117  low-confidence     4899.0  \n",
       "120  low-confidence     5069.0  \n",
       "132  low-confidence     5571.0  \n",
       "141  low-confidence     5967.0  \n",
       "144  low-confidence     6058.0  \n",
       "152  low-confidence     6467.0  \n",
       "156  low-confidence     6608.0  \n",
       "161  low-confidence     6860.0  \n",
       "163  low-confidence     7065.0  \n",
       "166  low-confidence     7191.0  \n",
       "167  low-confidence     7202.0  \n",
       "169  low-confidence     7275.0  \n",
       "170  low-confidence     7313.0  \n",
       "181  low-confidence     7614.0  \n",
       "184  low-confidence     7763.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hom_data_eaf_video_gentle_seg[hom_data_eaf_video_gentle_seg.seg_merging == \"low-confidence\"][[\"word\", \"prev_word\", \"seg_prev_word\", \"next_word\", \"seg_next_word\", \"seg_merging\", 'seg_index']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
